## Chinese

### å°ˆæ¡ˆæ¦‚è¿°
AnoniMe æ˜¯ä¸€æ¬¾æ¡Œé¢æ–‡ä»¶å»è­˜åˆ¥åŒ–æ‡‰ç”¨ç¨‹å¼ï¼Œèƒ½è‡ªå‹•æª¢æ¸¬ä¸¦æ›¿æ›æ–‡ä»¶ä¸­çš„å€‹äººè­˜åˆ¥è³‡è¨Šï¼ˆPIIï¼‰ã€‚æ¡ç”¨ PySide6 å’Œ QML å»ºæ§‹ï¼Œæä¾›å‹å–„çš„ä½¿ç”¨è€…ä»‹é¢ä¾†è™•ç† TXTã€DOCX å’Œ PDF æª”æ¡ˆï¼ŒåŒæ™‚ä¿æŒæ–‡ä»¶çµæ§‹å’Œæ ¼å¼ã€‚

### ä¸»è¦åŠŸèƒ½
- **å¤šæ ¼å¼æ”¯æ´**ï¼šè™•ç† TXTã€DOCX å’Œ PDF æª”æ¡ˆ
- **é€²éš PII æª¢æ¸¬**ï¼šåŸºæ–¼ Microsoft Presidio ä¸¦çµåˆå°ç£ç‰¹æœ‰è­˜åˆ¥å™¨
- **æ™ºæ…§æ›¿æ›**ï¼šä½¿ç”¨ Faker å‡½å¼åº«é€²è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å‡è³‡æ–™ç”Ÿæˆ
- **æ–‡ä»¶é è¦½**ï¼šå…§å»ºè™•ç†å¾Œæ–‡ä»¶çš„é è¦½åŠŸèƒ½
- **å¤šèªè¨€æ”¯æ´**ï¼šæ”¯æ´ä¸­è‹±æ–‡èªè¨€è™•ç†

### æ¶æ§‹èªªæ˜

```
AnoniMe/
â”œâ”€â”€ main.py                    # ä¸»ç¨‹å¼é€²å…¥é»
â”œâ”€â”€ Main.qml                   # QML ä½¿ç”¨è€…ä»‹é¢ä¸»è¦–çª—
â”œâ”€â”€ HomePage.qml               # é¦–é ä»‹é¢
â”œâ”€â”€ UploadPage.qml            # æª”æ¡ˆä¸Šå‚³ä»‹é¢
â”œâ”€â”€ ResultPage.qml            # çµæœé¡¯ç¤ºä»‹é¢
â”œâ”€â”€ EmbedViewer.qml           # æ–‡ä»¶é è¦½å…ƒä»¶
â”œâ”€â”€ test_backend.py           # å¢å¼·ç‰ˆå¾Œç«¯å«é è¦½åŠŸèƒ½
â”œâ”€â”€ pii_models/               # PII æª¢æ¸¬æ¨¡çµ„
â”‚   â”œâ”€â”€ presidio_detector.py  # æ ¸å¿ƒ PII æª¢æ¸¬å¼•æ“
â”‚   â”œâ”€â”€ custom_recognizer_plus.py # å°ç£ç‰¹æœ‰è­˜åˆ¥å™¨
â”‚   â””â”€â”€ detector.py           # æª¢æ¸¬å·¥å…·
â”œâ”€â”€ faker_models/             # è³‡æ–™æ›¿æ›æ¨¡çµ„
â”‚   â”œâ”€â”€ presidio_replacer.py  # ä¸»è¦æ›¿æ›å¼•æ“
â”‚   â””â”€â”€ tony_faker.py         # è‡ªè¨‚å‡è³‡æ–™ç”¢ç”Ÿå™¨
â”œâ”€â”€ file_handlers/            # æª”æ¡ˆè™•ç†æ¨¡çµ„
â”‚   â”œâ”€â”€ txt_handler.py        # æ–‡å­—æª”è™•ç†å™¨
â”‚   â”œâ”€â”€ docx_handler.py       # Word æ–‡ä»¶è™•ç†å™¨
â”‚   â””â”€â”€ pdf_handler_1.py      # PDF è™•ç†å™¨
â””â”€â”€ scripts/                  # å·¥å…·è…³æœ¬
    â””â”€â”€ minimal_text_demo.py  # ç¤ºç¯„è…³æœ¬
```

### æ ¸å¿ƒæŠ€è¡“

#### PII æª¢æ¸¬ç®¡ç·š
```python
# presidio_detector.py - å¤šèªè¨€ PII æª¢æ¸¬
from presidio_analyzer import AnalyzerEngine, RecognizerRegistry
from presidio_analyzer.nlp_engine import NlpEngineProvider

nlp_config = {
    "nlp_engine_name": "spacy",
    "models": [
        {"lang_code": "en", "model_name": "en_core_web_sm"},
        {"lang_code": "zh", "model_name": "zh_core_web_sm"},
    ],
}

provider = NlpEngineProvider(nlp_configuration=nlp_config)
nlp_engine = provider.create_engine()
analyzer = AnalyzerEngine(nlp_engine=nlp_engine, supported_languages=["en", "zh"])
```

#### å°ç£è‡ªè¨‚è­˜åˆ¥å™¨
```python
# custom_recognizer_plus.py - å°ç£ç‰¹æœ‰å¯¦é«”è­˜åˆ¥
def validate_tw_ubn(ubn: str) -> bool:
    """å°ç£çµ±ä¸€ç·¨è™Ÿæª¢é©—å™¨"""
    if not re.fullmatch(r"\d{8}", ubn):
        return False
    coef = [1,2,1,2,1,2,4,1]
    s = 0
    for i, c in enumerate(ubn):
        p = int(c) * coef[i]
        s += (p // 10) + (p % 10)
    return s % 10 == 0 or (s + 1) % 10 == 0

# æ”¯æ´çš„å°ç£å¯¦é«”ï¼š
# - èº«åˆ†è­‰å­—è™Ÿ
# - çµ±ä¸€ç·¨è™Ÿï¼ˆå…¬å¸è¡Œè™Ÿï¼‰
# - å°ç£é›»è©±è™Ÿç¢¼
# - ç¶²è·¯å¡ä½å€
```

#### æ™ºæ…§è³‡æ–™æ›¿æ›
```python
# presidio_replacer.py - ä¸Šä¸‹æ–‡æ„ŸçŸ¥å‡è³‡æ–™ç”Ÿæˆ
from presidio_anonymizer import AnonymizerEngine
from faker import Faker

def replace_pii(text, analyzer_results):
    """å°‡æª¢æ¸¬åˆ°çš„ PII æ›¿æ›ç‚ºç¬¦åˆä¸Šä¸‹æ–‡çš„å‡è³‡æ–™"""
    anonymizer = AnonymizerEngine()
    
    # å°ç£ç‰¹æœ‰å¯¦é«”çš„è‡ªè¨‚æ“ä½œå™¨
    operators = {
        "TW_ID": OperatorConfig("custom", {"lambda": fake_tw_id}),
        "TW_UBN": OperatorConfig("custom", {"lambda": fake_ubn}),
        "PHONE_NUMBER": OperatorConfig("custom", {"lambda": fake_phone}),
    }
    
    return anonymizer.anonymize(
        text=text,
        analyzer_results=analyzer_results,
        operators=operators
    )
```

### API å¥‘ç´„

#### è¼¸å…¥ JSON çµæ§‹
```json
{
  "file_path": "æª”æ¡ˆè·¯å¾‘å­—ä¸²",
  "language": "en|zh|auto",
  "options": {
    "name": true,
    "email": true,
    "phone": true,
    "id_number": true,
    "address": true
  }
}
```

#### è¼¸å‡º JSON çµæ§‹
```json
{
  "status": "success|error",
  "original_file": "åŸå§‹æª”æ¡ˆè·¯å¾‘",
  "processed_file": "è™•ç†å¾Œæª”æ¡ˆè·¯å¾‘",
  "preview_file": "é è¦½æª”æ¡ˆè·¯å¾‘",
  "entities_found": [
    {
      "entity_type": "å¯¦é«”é¡å‹",
      "text": "åŒ¹é…æ–‡å­—",
      "start": "èµ·å§‹ä½ç½®",
      "end": "çµæŸä½ç½®",
      "confidence": "ä¿¡å¿ƒåˆ†æ•¸"
    }
  ],
  "processing_time": "è™•ç†æ™‚é–“ï¼ˆç§’ï¼‰",
  "error_message": "éŒ¯èª¤è¨Šæ¯"
}
```

### å®‰è£èˆ‡ä½¿ç”¨

#### ç’°å¢ƒéœ€æ±‚
```bash
pip install PySide6 presidio-analyzer presidio-anonymizer spacy faker PyMuPDF python-docx
python -m spacy download en_core_web_sm
python -m spacy download zh_core_web_sm
```

#### åŸ·è¡Œæ‡‰ç”¨ç¨‹å¼
```bash
# æ¨™æº–æ¨¡å¼
python main.py

# å¢å¼·æ¨¡å¼å«é è¦½åŠŸèƒ½
python run_with_test_backend.py
```

#### æœ€å°æ¸¬è©¦è…³æœ¬
```bash
python scripts/minimal_text_demo.py
```

### æ¸¬è©¦
```bash
# åŸ·è¡Œå®Œæ•´æ¸¬è©¦
python simple_test.py

# æ¸¬è©¦ç‰¹å®šæª”æ¡ˆè™•ç†å™¨
python test_file_routing.py

# å¾Œç«¯åŠŸèƒ½æ¸¬è©¦
python test_backend.py
```

---

## Dependencies | ç›¸ä¾å¥—ä»¶

- **PySide6**: Desktop application framework | æ¡Œé¢æ‡‰ç”¨ç¨‹å¼æ¡†æ¶
- **Microsoft Presidio**: PII detection and anonymization | PII æª¢æ¸¬èˆ‡åŒ¿ååŒ–
- **spaCy**: Natural language processing | è‡ªç„¶èªè¨€è™•ç†
- **Faker**: Fake data generation | å‡è³‡æ–™ç”Ÿæˆ
- **PyMuPDF**: PDF processing | PDF è™•ç†
- **python-docx**: Word document processing | Word æ–‡ä»¶è™•ç†

## License | æˆæ¬Š
This project is licensed under the MIT License | æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾
  - `start: int`, `end: int` â€” åŸæ–‡ä¸­çš„å­—å…ƒèµ·è¿„ä½ç½®
  - `score: float` â€” ä¿¡å¿ƒåˆ†æ•¸
  - `raw_txt: str` â€” åŸå§‹ç‰‡æ®µ

- å‚³å›çµ¦ QML çš„å¾Œç«¯çµæœï¼ˆ`resultsReady` ä¸­æ¯å€‹å…ƒç´ ï¼‰ï¼š
  - `fileName: str` â€” è¼¸å‡ºæª”å
  - `type: "text" | "docx" | "pdf" | "binary"`
  - `originalText: str` â€”ï¼ˆå¯é¸ï¼‰åŸæ–‡é è¦½
  - `maskedText: str` â€”ï¼ˆå¯é¸ï¼‰æ›¿æ›å¾Œæ–‡å­—é è¦½
  - `embedData: object` â€” ä¾› `EmbedViewer.qml` ä½¿ç”¨çš„é è¦½è³‡æ–™ï¼Œä¾‹å¦‚ï¼š
    - æ–‡å­—ï¼š`{ viewType: "text", content: str, syntaxType: str, lineCount: int }`
    - PDFï¼š`{ viewType: "pdf", pageImages: string[], pageCount: int, metadata?: object }`
  - `outputPath?: str` â€” å»è­˜åˆ¥åŒ–å¾Œæª”æ¡ˆçš„çµ•å°è·¯å¾‘

- æ–‡å­—è™•ç†å™¨ APIï¼š
  - `TextHandler.deidentify(input_path: str, output_path: str, language: str = "auto") -> str`
  - å›å‚³å¯¦éš›å¯«å…¥å®Œæˆçš„ `output_path`ã€‚

å¿«é€Ÿç¤ºç¯„ï¼ˆä¸­æ–‡ï¼‰ï¼š

```powershell
python scripts/minimal_text_demo.py --mode detect-replace
python scripts/minimal_text_demo.py --mode file
```
- UIï¼šPySide6 + QMLï¼ˆ`Main.qml`/`HomePage.qml`/`UploadPage.qml`/`ResultPage.qml`ï¼‰
- å¾Œç«¯ï¼š`main.py`ï¼ˆæ­£å¼æµç¨‹ï¼‰èˆ‡ `test_backend.py`ï¼ˆé è¦½å¼·åŒ–ç‰ˆï¼‰
- PII åµæ¸¬ï¼šMicrosoft Presidioï¼ˆspacy å¤šèªï¼‰+ è‡ªè¨‚è¾¨è­˜å™¨
- å‡è³‡æ–™æ›¿æ›ï¼šPresidio Anonymizer + Fakerï¼›PDF å¦å«ä¸€çµ„ Faker æ›¿æ›ç®¡ç·š

## ç›®éŒ„ç¸½è¦½

```
.
â”œâ”€ main.py                         # æ­£å¼ Backendï¼ˆæª”æ¡ˆè·¯ç”±ã€é è¦½è³‡æ–™ã€è½‰æª”ï¼‰
â”œâ”€ run_with_test_backend.py        # å•Ÿå‹• QML + æ¸¬è©¦å¾Œç«¯
â”œâ”€ test_backend.py                 # æ¸¬è©¦ Backendï¼šçµ±ä¸€ç”¢ç”Ÿ PDF é è¦½èˆ‡é åœ–å›å‚³
â”œâ”€ Main.qml / HomePage.qml / UploadPage.qml / ResultPage.qml / EmbedViewer.qml / MaskCheckBox.qml
â”œâ”€ file_handlers/                  # å„æ ¼å¼è™•ç†å™¨
â”‚  â”œâ”€ txt_handler.py               # ç´”æ–‡å­—åµæ¸¬â†’æ›¿æ›â†’è¼¸å‡º
â”‚  â”œâ”€ docx_handler.py              # éæ­· runsã€è¡¨æ ¼ cells åµæ¸¬â†’æ›¿æ›â†’è¼¸å‡º DOCX
â”‚  â”œâ”€ pdf_handler.py               # é€ span åµæ¸¬â†’æ›¿æ›â†’è¼¸å‡º PDF
â”‚  â””â”€ pdf_handler_1.py             #ï¼ˆè®Šé«”ï¼‰ä½¿ç”¨ faker mapping æ›¿æ›
â”‚
â”œâ”€ pii_models/                     # PII åµæ¸¬
â”‚  â”œâ”€ presidio_detector.py         # Presidio AnalyzerEngineï¼ˆspacy å¤šèªï¼‰+ detect_pii()
â”‚  â”œâ”€ custom_recognizer_plus.py    # å°ç£å¸¸è¦‹ï¼šèº«åˆ†è­‰/çµ±ç·¨/æ‰‹æ©Ÿ/å¸‚è©±/MAC/å¥ä¿å¡â€¦ï¼ˆregex + context + æ ¡é©—ï¼‰
â”‚  â”œâ”€ custom_recognizer.py         # ç²¾ç°¡è‡ªè¨‚è­˜åˆ¥å™¨ç‰ˆæœ¬
â”‚  â””â”€ detector.py                  # spacy + regex çš„å¦ä¸€æ¢è·¯ï¼ˆæœªé€²ä¸»æµç¨‹ï¼‰
â”‚
â”œâ”€ faker_models/                   # å‡è³‡æ–™æ›¿æ›
â”‚  â”œâ”€ presidio_replacer.py         # Presidio Anonymizer + Fakerï¼ˆä»¥ entity type æ±ºå®šæ›¿æ›ç­–ç•¥ï¼‰
â”‚  â””â”€ tony_faker.py                # ä¾åµæ¸¬çµæœç”¢ç”Ÿå°æ‡‰å‡å€¼ã€å–æœ€é«˜åˆ†ã€æ˜ å°„æ›¿æ›
â”œâ”€ scripts/                        # å–®æª”æ¸¬è©¦è…³æœ¬
â”‚  â”œâ”€ run_txt_file.py
â”‚  â”œâ”€ run_docx_file.py
â”‚  â””â”€ run_pdf_file.py
â”‚
â””â”€ test_output/                    # è™•ç†çµæœèˆ‡é è¦½
  â”œâ”€ *_deid.(txt|docx|pdf)
  â””â”€ _previews/ ...
```

## å®‰è£èˆ‡åŸ·è¡Œï¼ˆWindows, PowerShellï¼‰

> éœ€æ±‚é‡é»ï¼šPython 3.10+ã€pipï¼›å»ºè­°å®‰è£ `en_core_web_sm` èˆ‡ `zh_core_web_sm` spacy æ¨¡å‹ã€‚PDF é è¦½ä½¿ç”¨ PyMuPDFï¼Œä¸éœ€é¡å¤–å­—å‹ã€‚

1) å»ºç«‹ç’°å¢ƒèˆ‡å®‰è£å¥—ä»¶

```powershell
python -m venv .venv
2) å•Ÿå‹•ï¼ˆå»ºè­°å…ˆç”¨æ¸¬è©¦å¾Œç«¯ï¼Œé è¦½æœ€ç©©å®šï¼‰

```powershell
# ä½¿ç”¨æ¸¬è©¦å¾Œç«¯ï¼šçµ±ä¸€å°‡çµæœè½‰æˆ PDF + é åœ–å›å‚³åˆ°å‰ç«¯
python run_with_test_backend.py
3) æ­£å¼å¾Œç«¯ï¼ˆè‹¥ä½ çš„ç’°å¢ƒæœ‰ Word æˆ– LibreOfficeï¼Œå¯ç”¨è¼ƒè²¼è¿‘æ­£å¼æµç¨‹çš„è½‰æª”ï¼‰

<<<<<<< HEAD
```powershell
python main.py
- DOCXâ†’PDF é è¦½ç­–ç•¥ï¼š
  - Windows + Wordï¼ˆpywin32 COMï¼‰å„ªå…ˆï¼›
  - å¤±æ•—å‰‡å˜—è©¦ LibreOfficeï¼ˆå°‡ `soffice.exe` åŠ å…¥ PATH æˆ–ä»¥ç’°å¢ƒè®Šæ•¸ `SOFFICE_PATH` æŒ‡å®šï¼‰ã€‚
- PDF handler ä¸­è‹¥æœ‰ç¡¬ç·¨ç¢¼å­—å‹è·¯å¾‘ï¼ˆmacOS ç¯„ä¾‹ï¼‰ï¼Œè«‹æ”¹ç‚ºç³»çµ±å¯ç”¨å­—å‹æˆ–ä½¿ç”¨æ¨™æº–å­—å‹åï¼ˆå¦‚ `helv`ï¼‰ã€‚

## ä½¿ç”¨æµç¨‹èˆ‡ UI

- åœ¨ `UploadPage` æ‹–æ”¾æˆ–é¸å–æª”æ¡ˆï¼Œå‹¾é¸è¦è™•ç†çš„é …ç›®ï¼ˆå§“å/Email/é›»è©±/IDâ€¦ï¼‰ï¼ŒæŒ‰ã€Œç”Ÿæˆçµæœã€ã€‚
- å¾Œç«¯æŠŠæª”æ¡ˆè·¯ç”±åˆ° `file_handlers/*_handler.py`ï¼Œåµæ¸¬â†’æ›¿æ›â†’è¼¸å‡ºåˆ° `test_output/`ã€‚
- å‰ç«¯é¡¯ç¤ºè™•ç†å¾Œæª”æ¡ˆçš„ PDF é åœ–æˆ–æ–‡å­—é è¦½ï¼ˆ`EmbedViewer.qml`ï¼‰ã€‚

## PII åµæ¸¬èˆ‡å‡è³‡æ–™æ›¿æ›ï¼ˆæ ¸å¿ƒè¨­è¨ˆï¼‰

æœ¬å°ˆæ¡ˆå°æ–‡å­—ã€DOCXã€PDF éƒ½ä½¿ç”¨ã€Œå…ˆåµæ¸¬ spanï¼Œå†æ›¿æ›ã€çš„ç­–ç•¥ï¼š

- åµæ¸¬ï¼š`pii_models/presidio_detector.py` å»ºç«‹ Presidio `AnalyzerEngine`ï¼Œè¼‰å…¥ `en_core_web_sm` / `zh_core_web_sm`ï¼Œä¸¦å‘¼å« `custom_recognizer_plus.register_custom_entities()` è¨»å†Šå°ç£å¸¸è¦‹ PII è­˜åˆ¥è¦å‰‡ï¼ˆå«æ ¡é©—èˆ‡èªå¢ƒå¼·åŒ–ï¼‰ã€‚
- æ›¿æ›ï¼š
  - æ–‡å­—/DOCXï¼š`faker_models/presidio_replacer.py` ä»¥ Presidio Anonymizer + Fakerï¼Œä¾ `entity_type` æä¾›åˆç†å‡å€¼æˆ–é®è”½ã€‚
  - PDFï¼š`file_handlers/pdf_handler_1.py` æœƒå…ˆä»¥ `tony_faker.py` é‡å°åµæ¸¬çµæœç”¢ç”Ÿå‡å€¼ mappingï¼Œå†ä¾ span å®šä½æ›¿æ›æ–‡å­—ï¼Œä¿æŒåŸåº§æ¨™èˆ‡å­—ç´šã€‚

### Detectï¼š`pii_models/presidio_detector.py`

```python
# å»ºç«‹ spacy å¤šèªå¼•æ“ + Presidio Analyzer
nlp_config = {
  "nlp_engine_name": "spacy",
  "models": [
    {"lang_code": "en", "model_name": "en_core_web_sm"},
    {"lang_code": "zh", "model_name": "zh_core_web_sm"},
  ],
}
provider = NlpEngineProvider(nlp_configuration=nlp_config)
nlp_engine = provider.create_engine()

analyzer = AnalyzerEngine(
  nlp_engine=nlp_engine,
  supported_languages=["en", "zh"]
)

# è¨»å†Šè‡ªè¨‚å¯¦é«”ï¼ˆå°ç£èº«åˆ†è­‰ã€çµ±ç·¨ã€æ‰‹æ©Ÿ/å¸‚è©±ã€MACã€å¥ä¿å¡â€¦ï¼‰
register_custom_entities(analyzer)

def detect_pii(text: str, language: str = "auto", score_threshold: float = 0.5):
  results = analyzer.analyze(text=text, entities=None, language=language)
  # æ•´ç†ç‚ºçµ±ä¸€ dict æ ¼å¼ï¼ˆå« raw_txtï¼‰
  filtered = []
  for r in results:
    if r.score >= score_threshold:
      filtered.append({
                "entity_type": r.entity_type,
                "start": r.start,
                "end": r.end,
        "score": r.score,
        "raw_txt": text[r.start:r.end]
      })
  return filtered
```

é‡é»ï¼š
- `register_custom_entities` åŠ å¼·å°ç£ç‰¹æœ‰å¯¦é«”ï¼ˆä¾‹å¦‚çµ±ç·¨å«æ ¡é©—ã€æ‰‹æ©Ÿæ”¯æ´ +886 å¤šç¨®æ ¼å¼ã€MAC æ­£è¦åŒ–èˆ‡ä½è³ªç‰¹ä¾‹é™åˆ†ï¼‰ã€‚
- å›å‚³ç‚ºçµ±ä¸€ dictï¼Œå¾ŒçºŒæ›¿æ›ç«¯å¯ä»¥ä¸ä¾è³´ Presidio çš„ç‰©ä»¶å‹åˆ¥ã€‚

### Custom Recognizersï¼š`pii_models/custom_recognizer_plus.py`

```python
# ä»¥ PatternRecognizer + context/validator å¼·åŒ–ï¼š
# - UNIFIED_BUSINESS_NOï¼šæ ¡é©—åˆæ³•çµ±ç·¨å¾Œæ‹‰é«˜ä¿¡å¿ƒï¼Œå¦å‰‡å£“ä½
# - TW_PHONE_NUMBERï¼šæ”¯æ´åœ‹éš›/æœ¬åœ°å¤šç¨®æ ¼å¼ï¼ˆ+886, 09xx-xxx-xxx, ...ï¼‰
# - TW_HOME_NUMBERï¼šå¸‚è©±ï¼ˆå«æ‹¬è™Ÿ/ç ´æŠ˜/åœ‹ç¢¼ï¼‰
# - MAC_ADDRESSï¼šæ”¯æ´å†’è™Ÿã€ç ´æŠ˜ã€Cisco dottedï¼›00..00 ç­‰ä¾‹å¤–é™åˆ†
# - TW_NHI_NUMBERï¼šå¥ä¿å¡ï¼ˆä»¥ context é¿å…èª¤æ“Šï¼‰

for lang in ("zh", "en"):
  analyzer.registry.add_recognizer(tw_id_recognizer)
  analyzer.registry.add_recognizer(tw_ubn_recognizer)
  analyzer.registry.add_recognizer(tw_phone_recognizer)
  analyzer.registry.add_recognizer(tw_home_recognizer)
  analyzer.registry.add_recognizer(mac_recognizer)
  analyzer.registry.add_recognizer(tw_nhi_recognizer)
```

é‡é»ï¼š
- ä»¥ contextï¼ˆé—œéµå­—ä¸Šä¸‹æ–‡ï¼‰é™ä½ä¸€èˆ¬æ•¸å­—ä¸²èª¤åˆ¤ã€‚
- UBN é€é checksum é©—è­‰æ§åˆ¶åˆ†æ•¸ï¼Œæå‡ç²¾ç¢ºåº¦ã€‚

### Replaceï¼ˆæ–‡å­—/DOCXï¼‰ï¼š`faker_models/presidio_replacer.py`

```python
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import OperatorConfig
from presidio_analyzer import RecognizerResult
from faker import Faker

anonymizer = AnonymizerEngine()
fake = Faker()

def replace_pii(text: str, analyzer_results: list[dict]) -> str:
  # å°‡åµæ¸¬çµæœï¼ˆdictï¼‰è½‰ Presidio RecognizerResult
  recognizer_results = [
    RecognizerResult(
      entity_type=r["entity_type"], start=r["start"], end=r["end"], score=r["score"]
    ) for r in analyzer_results
  ]

  # ä¾ entity æ±ºå®šæ›¿æ›ç­–ç•¥ï¼ˆç¤ºæ„ï¼‰
  operators = {
    "EMAIL_ADDRESS": OperatorConfig("replace", {"new_value": "user@example.com"}),
    "PHONE_NUMBER": OperatorConfig("replace", {"new_value": fake.phone_number()}),
    "PERSON":       OperatorConfig("replace", {"new_value": fake.name()}),
    "LOCATION":     OperatorConfig("replace", {"new_value": fake.address()}),
    "IP_ADDRESS":   OperatorConfig("replace", {"new_value": fake.ipv4()}),
    "CREDIT_CARD":  OperatorConfig("replace", {"new_value": fake.credit_card_number()}),
    # å°ç£å¸¸è¦‹ï¼š
    "TW_ID_NUMBER":         OperatorConfig("replace", {"new_value": _fake_tw_id()}),
    "UNIFIED_BUSINESS_NO":  OperatorConfig("replace", {"new_value": _fake_ubn()}),
    "TW_PHONE_NUMBER":      OperatorConfig("replace", {"new_value": _fake_tw_mobile()}),
    # å…¶ä»–æœªçŸ¥å‹åˆ¥ï¼šä¿ç•™åŸæ–‡æˆ–ä»¥ã€â˜…ã€é®è”½
  }

  return anonymizer.anonymize(
    text=text, analyzer_results=recognizer_results, operators=operators
  ).text
```

é‡é»ï¼š
- ä½¿ç”¨ Presidio Anonymizer çš„ OperatorConfigï¼Œä»¥ã€Œæ›¿æ›ã€ç‚ºä¸»ï¼Œä¹Ÿå¯æ”¹ maskã€‚
- å°åœ°å€ç‰¹æœ‰ï¼ˆTWï¼‰æä¾›å°ˆå±¬ç”¢ç”Ÿå™¨ï¼Œä»¥ä¿æŒæ ¼å¼åˆç†æ€§ã€‚

### Replaceï¼ˆPDFï¼‰ï¼š`file_handlers/pdf_handler_1.py`

```python
# é€é é€ spanï¼šå…ˆ detectï¼Œå†ç”¨ faker ç”Ÿæˆå°æ‡‰ fake_mapï¼Œä¾åŸèµ·è¨–ä½ç½®æ›¿æ›æ–‡å­—ï¼Œ
# ä¸¦ç¶­æŒåŸæœ‰ bbox / å­—ç´šï¼Œæœ€å¾Œä»¥ PyMuPDF é‡å»ºæ–° PDFã€‚
entities = detect_pii(text, language="en", score_threshold=0.6)
faker_results = test_all_methods(entities)
best_results = keep_highest_score_per_raw_txt(faker_results)
fake_map = {item["raw_txt"]: item["fake_value"] for item in best_results}

masked_text = text
offset = 0
for ent in entities:
  start, end = ent["start"] + offset, ent["end"] + offset
  raw_txt = ent["raw_txt"]
  fake_value = fake_map.get(raw_txt, "*" * (end - start))
  fake_value = fake_value[:len(raw_txt)].ljust(len(raw_txt))
  masked_text = masked_text[:start] + fake_value + masked_text[end:]
  offset += len(fake_value) - (end - start)
```

é‡é»ï¼š
- ä»¥ `offset` è™•ç†æ›¿æ›å¾Œé•·åº¦å·®ï¼Œé¿å…å¾ŒçºŒ span ä½ç½®éŒ¯ä½ã€‚
- å…·é«”æ›¿æ›ç­–ç•¥å¯æ’æ‹”ï¼šå¯æ”¹ç‚ºå…¨é®è”½æˆ–æŒ‰é¡å‹ç”¢å€¼å¾—æ›´ç²¾ç´°ã€‚

## å¾Œç«¯è·¯ç”±èˆ‡è¼¸å‡º

`main.Backend._process_file_with_deidentification()` ä¾å‰¯æª”åè·¯ç”±ï¼š

- text â†’ `TextHandler.deidentify()` â†’ `*_deid.txt`
- docx â†’ `DocxHandler.deidentify()` â†’ `*_deid.docx`
- pdf â†’ `PdfHandler.deidentify()` â†’ `*_deid.pdf`

é è¦½ï¼š
- PDF ç›´æ¥è½‰é åœ–ï¼ˆPyMuPDFï¼‰ã€‚
- DOC/DOCX å˜—è©¦è½‰ PDF å†è½‰é åœ–ï¼›è‹¥ç„¡ Word/LibreOfficeï¼Œæ”¹ç‚º unsupported è¨Šæ¯ã€‚
- TXT ç›´æ¥æä¾›å…§å®¹é è¦½ï¼ˆè¡Œè™Ÿ/èªæ³•è‰²åº•ï¼‰ã€‚

## å¸¸è¦‹å•é¡Œï¼ˆFAQï¼‰

- spacy æ¨¡å‹ä¸‹è¼‰éŒ¯èª¤ï¼Ÿè«‹ç¢ºèªç¶²è·¯æˆ–æ”¹ç”¨é›¢ç·šå®‰è£ï¼Œç¢ºä¿ `en_core_web_sm`ã€`zh_core_web_sm` å¯ç”¨ã€‚
- PDF handler å‡ºç¾å­—å‹è·¯å¾‘éŒ¯èª¤ï¼Ÿå°‡ç¡¬ç·¨ç¢¼å­—å‹æ”¹ç‚ºæœ¬æ©Ÿå¯ç”¨æª”æ¡ˆï¼Œæˆ–ç°¡åŒ–ç‚ºæ¨™æº–å­—å‹å `helv`/`times`ã€‚
- DOCXâ†’PDF è½‰æª”å¤±æ•—ï¼Ÿ
  - Windows + Wordï¼ˆpywin32ï¼‰è¼ƒç©©å®šï¼›
  - ç„¡ Word æ™‚è«‹å®‰è£ LibreOfficeï¼Œä¸¦å°‡ `soffice.exe` åŠ å…¥ PATH æˆ–ä»¥ `SOFFICE_PATH` æŒ‡å®šã€‚

## æˆæ¬Š

æ­¤å°ˆæ¡ˆåŒ…å«ç¬¬ä¸‰æ–¹å¥—ä»¶ï¼ˆPresidioã€spaCyã€PyMuPDFã€Faker ç­‰ï¼‰ï¼Œå…¶æˆæ¬Šæ¢æ¬¾è«‹ä¾åŸå°ˆæ¡ˆç‚ºæº–ã€‚

# EdgeDeID Studio

EdgeDeID Studio is a real-time, on-device personal data anonymization toolkit that detects and redacts sensitive information (PII) from PDF documents, images, and tabular data within **150 ms**.

## âœ¨ Features

- ğŸ” **NER + OCR PII Detection**: Identifies names, emails, addresses, ID numbers, and more.
- ğŸ§  **Generative AI Augmentation**: Replace redacted info with synthetic names, or generate summaries.
- ğŸ“„ **Document Support**: Works with PDF, image, and CSV/Excel files.
- âš¡ **Edge-Optimized**: Quantized ONNX models run on Qualcomm Copilot+ NPU with <150ms latency.
- ğŸ›¡ï¸ **Privacy-First**: Everything runs locally. No data leaves the device.

## ğŸ§° Tech Stack

- **NER model**: `ckiplab/bert-base-chinese-ner`
- **Fake data generation**: `uer/gpt2-chinese-cluecorpussmall`
- **PDF/Image parsing**: `PyMuPDF`, `Pillow`, `pandas`
- **ONNX Inference**: `onnx`, `onnxruntime`, `onnxsim`
- **UI**: PySide6 (for graphical interface)

## ğŸ—‚ï¸ Project Structure

## PII Models
"""
```

### ğŸ§° [predidio](https://github.com/microsoft/presidio)
#### [Demo](https://huggingface.co/spaces/presidio/presidio_demo)

- Data Protection and De-identification SDK
- æ•ˆæœä½³

#### é›£é»
- å¤šç¨®èªè¨€é›£ä¸€æ¬¡åµæ¸¬(é™¤éç›´æ¥ä½¿ç”¨å¤šèª PII NER æ¨¡å‹åµæ¸¬)
- Spacy ä¸€æ¬¡åªèƒ½åµæ¸¬ä¸€ç¨®èªè¨€ (éœ€è¦å¤šæ¬¡å‘¼å« -> æ•ˆèƒ½ bad bad | ä½¿ç”¨è€…ç«¯é å…ˆé¸æ“‡ input file çš„èªè¨€)

### ğŸ§° [Multilingual NER](https://huggingface.co/Babelscape/wikineural-multilingual-ner)
- mBERT multilingual language model
- model is trained on WikiNEuRal (Therefore, it might not generalize well to all textual genres (e.g. news))

### ğŸ§° [xlm-roberta-base-ner-hrl](https://huggingface.co/Davlan/xlm-roberta-base-ner-hrl)
- based on a fine-tuned XLM-RoBERTa base model

### ğŸ§° [piiranha-v1-detect-personal-information](https://huggingface.co/iiiorg/piiranha-v1-detect-personal-information)
- open in Colab å¯ä»¥ç›´æ¥å¯¦æ¸¬
-

ä¸‹é¢å½™æ•´å¾æœ€åˆåˆ°ç›®å‰ï¼Œæˆ‘å€‘åœ¨ **EdgeDeID Studio** å°ˆæ¡ˆä¸­æ‰€å¯¦ä½œçš„å…¨éƒ¨åŠŸèƒ½ã€æª”æ¡ˆçµæ§‹èˆ‡æ¸¬è©¦ç­–ç•¥ï¼Œä¸¦èªªæ˜æ¯å€‹æ¨¡çµ„å¦‚ä½•ä¸²æ¥æˆã€Œå»è­˜åˆ¥åŒ–ï¼‹æ›¿æ›å‡è³‡æ–™ã€çš„å®Œæ•´æµç¨‹ï¼Œä»¥åŠæˆ‘å€‘å¦‚ä½•ç”Ÿæˆï¼†æ‡‰ç”¨æ•æ„Ÿå‡è³‡æ–™ã€‚

---

## ä¸€ã€å°ˆæ¡ˆç›®éŒ„ç¸½è¦½

```
/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ regex_zh.yaml                   # ä¸­æ–‡æ­£å‰‡è¦å‰‡
â”‚
â”œâ”€â”€ models/                             # åŸå§‹ Hugging Face æ¨¡å‹å¿«å–
â”‚   â”œâ”€â”€ ner/bert-ner-zh/
â”‚   â””â”€â”€ gpt2/
â”‚
â”œâ”€â”€ edge_models/
â”‚   â””â”€â”€ bert-ner-zh.onnx                # ONNX æ ¼å¼ NER æ¨¡å‹
â”‚
â”œâ”€â”€ scripts/                            # å„ç¨®å·¥å…·è…³æœ¬
â”‚   â”œâ”€â”€ download_models.py              # ä¸€éµä¸‹è¼‰ HF æ¨¡å‹
â”‚   â”œâ”€â”€ run_automated_pipeline.py       # è‡ªå‹•åŒ–æ¸¬è©¦ç®¡ç·š
â”‚   â”œâ”€â”€ benchmark_formats.py            # æ ¼å¼æ•ˆèƒ½åŸºæº–æ¸¬è©¦
â”‚   â””â”€â”€ validate_quality.py             # å»è­˜åˆ¥åŒ–å“è³ªé©—è­‰
â”‚
â”œâ”€â”€ examples/                           # ä½¿ç”¨ç¯„ä¾‹
â”‚   â”œâ”€â”€ usage_examples.py               # åŸºæœ¬ & æ‰¹é‡è³‡æ–™ç”Ÿæˆç¤ºç¯„
â”‚   â””â”€â”€ advanced_usage.py               # é€²éšä½¿ç”¨ç¯„ä¾‹
â”‚
â”œâ”€â”€ sensitive_data_generator/           # å‡è³‡æ–™ç”Ÿæˆå­ç³»çµ±
â”‚   â”œâ”€â”€ __init__.py                     # å¥—ä»¶åŒ¯å‡ºä»‹é¢
â”‚   â”œâ”€â”€ config.py                       # åœ°å€ã€è¡—é“ã€å§“åã€é†«é™¢ç­‰è¨­å®š
â”‚   â”œâ”€â”€ generators.py                   # å„é¡ PII Generator
â”‚   â”œâ”€â”€ formatters.py                   # åŸºæœ¬æ®µè½ & æ–‡ä»¶æ¨¡æ¿
â”‚   â”œâ”€â”€ advanced_formatters.py          # é€²éšåˆç´„ï¼é†«ç™‚å ±å‘Šï¼è²¡å‹™å ±è¡¨æ¨¡æ¿
â”‚   â”œâ”€â”€ file_writers.py                 # åŸºæœ¬ TXT/PDF/Image/CSV/JSON è¼¸å‡º
â”‚   â”œâ”€â”€ advanced_file_writers.py        # é€²éš PDF/Word/Excel/PPT/æƒææª”è¼¸å‡º
â”‚   â””â”€â”€ dataset_generator.py            # ä¸€éµç”¢å‡ºå¤šæ ¼å¼æ¸¬è©¦è³‡æ–™é›†
â”‚
â”œâ”€â”€ src/deid_pipeline/                  # æ ¸å¿ƒ De-ID Pipeline
â”‚   â”œâ”€â”€ __init__.py                     # åŒ¯å‡º DeidPipeline é¡
â”‚   â”œâ”€â”€ config.py                       # Pipeline å…¨åŸŸè¨­å®š
â”‚   â”œâ”€â”€ parser/                         # æª”æ¡ˆæ–‡å­—æŠ½å–
â”‚   â”‚   â”œâ”€â”€ ocr.py                      # EasyOCR singleton
â”‚   â”‚   â””â”€â”€ text_extractor.py           # PDF/DOCX/Image â†’ ç´”æ–‡å­—
â”‚   â”œâ”€â”€ image_deid/                     # å½±åƒå»è­˜åˆ¥åŒ–
â”‚   â”‚   â””â”€â”€ processor.py                # OCRâ†’Detectâ†’Replaceâ†’å›å¯«åœ–ç‰‡
â”‚   â”œâ”€â”€ pii/                            # PII åµæ¸¬ & å‡è³‡æ–™æ›¿æ›æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ detectors/                  # å„ç¨®åµæ¸¬å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ regex_detector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ spacy_detector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ bert_detector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ bert_onnx_detector.py
â”‚   â”‚   â”‚   â””â”€â”€ composite.py            # å¤š detector çµæœåˆä½µ
â”‚   â”‚   â””â”€â”€ utils/                      # å…±ç”¨å·¥å…·
â”‚   â”‚       â”œâ”€â”€ base.py                 # Entity, PIIDetector æŠ½è±¡é¡
â”‚   â”‚       â”œâ”€â”€ fake_provider.py        # GPT-2 + Faker å‡è³‡æ–™ç”¢ç”Ÿå™¨
â”‚   â”‚       â””â”€â”€ replacer.py             # æ–‡æœ¬ & äº‹ä»¶è¨˜éŒ„å–ä»£é‚è¼¯
â”‚
â””â”€â”€ tests/                              # å„å±¤æ¸¬è©¦
    â”œâ”€â”€ test_data_factory.py            # Faker æ¸¬è©¦è³‡æ–™ç”¢ç”Ÿ
    â”œâ”€â”€ pii_test_suite.py               # Regex/BERT/Composite/Replacer å–®å…ƒ
    â”œâ”€â”€ test_detectors.py               # å¤š detector åƒæ•¸åŒ–æ¸¬è©¦
    â”œâ”€â”€ test_replacer.py                # æ›¿æ›ä¸€è‡´æ€§æ¸¬è©¦
    â”œâ”€â”€ test_onnx_speed.py              # ONNX å»¶é²åŸºæº– (<25ms)
    â”œâ”€â”€ integration_test.py             # extractâ†’detectâ†’replace æ•´åˆæ¸¬
    â”œâ”€â”€ performance_test.py             # ä¸åŒé•·åº¦æ–‡æœ¬æ•ˆèƒ½è¶¨å‹¢
    â”œâ”€â”€ end_to_end_test.py              # TXT/PDF/Image E2E æ¸¬è©¦
    â””â”€â”€ test_data_generator_integration.py  # å‡è³‡æ–™ç”Ÿæˆå™¨ + Pipeline æ•´åˆé©—è­‰
```

---

## ç’°å¢ƒå»ºç½®ï¼ˆEnvironment Setupï¼‰

æœ¬å°ˆæ¡ˆåŒæ™‚æä¾› Conda èˆ‡ Pip å…©ç¨®æ–¹å¼å»ºç«‹ç›¸åŒçš„é–‹ç™¼ç’°å¢ƒã€‚

### 1. ä½¿ç”¨ Conda

```bash
# 1) é€²å…¥åˆ°æœ¬å°ˆæ¡ˆæ ¹ç›®éŒ„
cd path/to/edge-deid-studio

# 2) å»ºç«‹ conda ç’°å¢ƒ
conda env create -f env/conda.yaml

# 3) å•Ÿå‹•ç’°å¢ƒ
conda activate edge-deid
````

> **æç¤º**ï¼šè‹¥ç’°å¢ƒåç¨±ä¸æ˜¯ `edge-deid`ï¼Œè«‹ç”¨ `conda env list` ç¢ºèªå¾Œå†åˆ‡æ›ã€‚

### 2. ä½¿ç”¨ Python venv + Pip

```bash
# 1) é€²å…¥åˆ°æœ¬å°ˆæ¡ˆæ ¹ç›®éŒ„
cd path/to/edge-deid-studio

# 2) å»ºç«‹ virtualenv
python3 -m venv .venv

# 3) å•Ÿå‹• venvï¼ˆLinux/macOSï¼‰
source .venv/bin/activate

#    Windows PowerShell
.\.venv\Scripts\Activate.ps1

# 4) å®‰è£æ‰€æœ‰ç›¸ä¾
pip install --upgrade pip
pip install -r requirements.txt
```

### 3. ç¢ºèªå®‰è£

```bash
# æ¸¬è©¦å¥—ä»¶
pytest --maxfail=1 --disable-warnings -q

# å¿«é€Ÿæª¢æŸ¥
python quick_tests.py

# å˜—è©¦ CLI
python main.py -i test_input/sample.pdf --mode replace --json
```

---

## äºŒã€æ ¸å¿ƒæ¨¡çµ„èˆ‡åŠŸèƒ½

### 1. De-ID Pipeline (`src/deid_pipeline/`)

* **`config.py`**
  ç®¡ç†æ¨¡å‹è·¯å¾‘ã€é–¾å€¼ã€OCR è¨­å®šã€Fake-data åƒæ•¸ã€ONNX é–‹é—œç­‰ã€‚
* **æ–‡å­—æŠ½å– (`parser/`)**

  * `text_extractor.py`ï¼šPDFï¼ˆ`fitz`ï¼‰ã€DOCXï¼ˆ`python-docx`ï¼‰ã€å½±åƒï¼ˆ`EasyOCR`ï¼‰â†’ çµ±ä¸€ `extract_text()`ã€‚
* **å½±åƒå»è­˜åˆ¥ (`image_deid/processor.py`)**
  OCR â†’ `get_detector()` åµæ¸¬ â†’ `Replacer.replace()` â†’ å¡—é»‘æˆ–æ›¿æ› â†’ å›å¯«åœ–ç‰‡ã€‚
* **PII åµæ¸¬ & å‡è³‡æ–™æ›¿æ› (`pii/`)**

  * **RegexDetector**ï¼šYAML è¦å‰‡ â†’ `re.finditer`ã€‚
  * **SpaCyDetector**ï¼šspaCy NER + regex è£œæ­£ã€‚
  * **BertDetector**ã€**BertONNXDetector**ï¼šSliding window â†’ Transformer æ¨è«–ã€‚
  * **Composite**ï¼šä¾ `ENTITY_PRIORITY` æ•´åˆå¤šæª¢æ¸¬å™¨çµæœã€‚
  * **FakeProvider**ï¼šGPT-2 + Faker fallback ç”Ÿæˆå‡å€¼ã€‚
  * **Replacer**ï¼šä¾ span åœ¨åŸæ–‡æ›¿æ›æˆ–å¡—é»‘ï¼Œä¸¦è¨˜éŒ„äº‹ä»¶ã€‚

æ•´åˆæˆ `DeidPipeline.process(input)` â†’ å›å‚³ `DeidResult(entities, output, report)`ã€‚

### Config.py åƒæ•¸ç¯„ä¾‹

```python
# src/deid_pipeline/config.py

# 1. è¦å‰‡æª”è·¯å¾‘
PROJECT_ROOT   = Path(__file__).resolve().parent.parent
CONFIGS_DIR    = PROJECT_ROOT / "configs"
REGEX_RULES_FILE = CONFIGS_DIR / "regex_zh.yaml"

def load_regex_rules(path: Path = REGEX_RULES_FILE) -> dict:
    with open(path, encoding="utf-8") as f:
        return yaml.safe_load(f)

class Config:
    """å…¨åŸŸè¨­å®šä¸­å¿ƒï¼šæ–‡å­—æŠ½å–ï¼PII åµæ¸¬ï¼å‡è³‡æ–™ç”Ÿæˆ"""

    # æ”¯æ´æª”æ¡ˆé¡å‹
    SUPPORTED_FILE_TYPES = [".pdf", ".docx", ".png", ".jpg"]

    # --- æ–‡å­—æŠ½å–è¨­å®š ---
    OCR_ENABLED      = True
    OCR_THRESHOLD    = 50
    OCR_LANGUAGES    = ["ch_tra", "en"]

    # --- BERT åµæ¸¬è¨­å®š ---
    NER_MODEL_PATH          = os.getenv("NER_MODEL_PATH", PROJECT_ROOT / "models" / "ner")
    BERT_CONFIDENCE_THRESHOLD = 0.85
    MAX_SEQ_LENGTH          = 512
    WINDOW_STRIDE           = 0.5
    ENTITY_PRIORITY = {
        "TW_ID": 100,
        "PASSPORT": 95,
        "PHONE": 85,
        "EMAIL": 80,
        "NAME": 75,
        "ADDRESS": 70,
    }

    # --- Regex è¦å‰‡ ---
    REGEX_PATTERNS = load_regex_rules()

    # --- å‡è³‡æ–™ç”Ÿæˆ ---
    GPT2_MODEL_PATH   = os.getenv("GPT2_MODEL_PATH", PROJECT_ROOT / "models" / "gpt2")
    FAKER_LOCALE      = "zh_TW"
    FAKER_CACHE_SIZE  = 1000

    # --- ONNX Runtime æ¨è«– ---
    USE_ONNX         = True
    ONNX_MODEL_PATH  = os.getenv("ONNX_MODEL_PATH", PROJECT_ROOT / "edge_models" / "bert-ner-zh.onnx")
    ONNX_PROVIDERS   = ["CPUExecutionProvider","CUDAExecutionProvider","NPUExecutionProvider"]

    # --- Logging & ç’°å¢ƒæ——æ¨™ ---
    ENVIRONMENT      = os.getenv("ENV", "local")
    LOG_LEVEL        = os.getenv("LOG_LEVEL", "INFO")
    ENABLE_PROFILING = False
    USE_STUB         = False
````

> **èªªæ˜**ï¼š
>
> * `OCR_*`ï¼šPDF æ–‡å­—æ“·å–çš„é–¾å€¼èˆ‡èªè¨€é…ç½®ï¼›
> * `NER_MODEL_PATH` ç­‰ï¼šBERT æ¨¡å‹è·¯å¾‘èˆ‡ sliding-window åƒæ•¸ï¼›
> * `REGEX_PATTERNS`ï¼šè¼‰å…¥ YAML å½¢å¼çš„ PII æ­£å‰‡ï¼›
> * `USE_ONNX`ï¼šåˆ‡æ›åˆ° ONNX Runtimeï¼›
> * å…¶é¤˜ç‚º Fake-dataã€Loggingã€ç’°å¢ƒæ§åˆ¶æ——æ¨™ã€‚


#### 1. Detector çµ„è£ (`detectors/__init__.py`)

```python
def get_detector(lang: str = "zh") -> CompositeDetector:
    config = Config()
    if lang == "zh" and not config.USE_STUB and MODEL_ZH.exists():
        return CompositeDetector(
            BertNERDetector(str(MODEL_ZH)),
            RegexDetector()
        )
    # çœç•¥å…¶ä»–åˆ†æ”¯â€¦â€¦
    else:
        return CompositeDetector(
            SpacyDetector(),
            RegexDetector(config_path="configs/regex_en.yaml")
        )
````

> **èªªæ˜**ï¼šå‹•æ…‹æŒ‘é¸ BERT/ONNX æˆ– SpaCy+Regexï¼Œä¸¦åŒ…æˆ CompositeDetectorã€‚

---

#### 2. Entity å®šç¾© (`utils/base.py`)

```python
class Entity(TypedDict):
    span: Tuple[int, int]     # åŸæ–‡ä¸­å­—å…ƒä½ç½® (start, end)
    type: PII_TYPES           # PII é¡å‹ï¼Œä¾‹å¦‚ NAMEã€IDã€PHONEâ€¦
    score: float              # åµæ¸¬ä¿¡å¿ƒå€¼
    source: str               # åµæ¸¬ä¾†æºï¼Œå¦‚ "bert", "regex", "spacy"
```

> **èªªæ˜**ï¼šç”¨ TypedDict å®šç¾©å¯åºåˆ—åŒ–çš„ PII å¯¦é«”çµæ§‹ï¼Œçµ±ä¸€æµè½‰æ ¼å¼ã€‚

---

#### 3. ç®¡ç·šå…¥å£ (`src/deid_pipeline/__init__.py`)

```python
class DeidPipeline:
    def __init__(self, language: str = "zh"):
        self.detector = get_detector(language)
        self.replacer = Replacer()
        self.ocr_proc = ImageDeidProcessor(lang=language)

    def process(self, input_path: str, output_mode: str = "replacement"):
        # 1. æ–‡å­—æˆ–å½±åƒæŠ½å–
        suffix = input_path.lower().rsplit(".", 1)[-1]
        if suffix in ("txt", "docx", "pdf"):
            text, _ = extract_text(input_path)
        else:
            ocr_res = self.ocr_proc.process_image(input_path)
            text = ocr_res["original_text"]

        # 2. åµæ¸¬
        entities = self.detector.detect(text)

        # 3. æ›¿æ›æˆ–å¡—é»‘
        clean_text, events = self.replacer.replace(text, entities)
        return DeidResult(entities=entities, text=clean_text)
```

> **èªªæ˜**ï¼šæ•´åˆæŠ½å–â†’åµæ¸¬â†’æ›¿æ›ä¸‰å¤§æ­¥é©Ÿï¼Œå°å¤–æä¾›ä¸€è‡´åŒ–ä»‹é¢ã€‚

---

### 2. å‡è³‡æ–™ç”Ÿæˆå­ç³»çµ± (`sensitive_data_generator/`)

#### a. åŸºæœ¬ç”Ÿæˆ

* **`config.py`**ï¼šå°ç£ç¸£å¸‚ã€è¡—é“ã€å§“æ°ã€åå­—ã€é†«é™¢ã€å°ˆç§‘æ¸…å–®ã€‚
* **`generators.py`**ï¼š

  * `generate_tw_id()`, `generate_tw_phone()`, `generate_tw_address()`, `generate_tw_name()`â€¦
  * `generate_random_pii()` éš¨æ©ŸæŒ‘é¸ä¸€ç¨® PIIã€‚
* **`formatters.py`**ï¼š

  * `generate_paragraph()`ï¼šè‡ªç„¶èªè¨€æ®µè½æ¨¡æ¿ï¼Œå…§åµŒ PIIã€å¯èª¿å¯†åº¦ã€‚
  * `generate_medical_record()`, `generate_financial_document()`, `generate_random_document()`ã€‚

#### b. é€²éšæ¨¡æ¿

* **`advanced_formatters.py`**ï¼š

  * `generate_contract_document()`ï¼šåˆç´„æ›¸ç¯„æœ¬ã€‚
  * `generate_medical_report()`ï¼šåœ–è¡¨å¼•ç”¨çš„é†«ç™‚å ±å‘Šæ®µè½ã€‚
  * `generate_financial_statement()`ï¼šMarkdown é¢¨æ ¼è²¡å‹™å ±è¡¨ã€‚

#### c. æª”æ¡ˆè¼¸å‡º

* **`file_writers.py`**ï¼š
  TXTã€ç°¡å–® PDFã€æ¨¡æ“¬æƒæåœ–ç‰‡ (PIL)ã€CSVã€JSONã€‚
* **`advanced_file_writers.py`**ï¼š

  * **ReportLab**ï¼šè¤‡é›œ PDFï¼ˆæ¨™é¡Œã€è¡¨æ ¼ã€åœ–è¡¨ï¼‰ã€‚
  * **python-docx**ï¼šWordï¼ˆæ¨™é¡Œã€è¡¨æ ¼ã€åœ–ç‰‡ã€é å°¾ï¼‰ã€‚
  * **xlsxwriter**ï¼šExcelï¼ˆæ ¼å¼åŒ– + åœ–è¡¨ï¼‰ã€‚
  * **python-pptx**ï¼šPPTï¼ˆæŠ•å½±ç‰‡ã€è¡¨æ ¼ã€åœ–ç‰‡ï¼‰ã€‚
  * **PIL**ï¼šæƒææ–‡ä»¶æ¨¡æ“¬ï¼ˆé›œè¨Šã€å°ç« ã€ç°½åï¼‰ã€‚

#### d. å¤šæ ¼å¼è³‡æ–™é›†ç”Ÿæˆ

* **`dataset_generator.py`**ï¼š
  `MultiFormatDatasetGenerator.generate_full_dataset(output_dir, num_items)`ï¼š

  1. åœ¨å„å­ç›®éŒ„ï¼ˆpdf/word/excel/ppt/scanned/contracts/medical/financialï¼‰ç”¢å‡ºå°æ‡‰æª”æ¡ˆã€‚
  2. åŒæ­¥å„²å­˜ç´”æ–‡å­—ç‰ˆã€‚
  3. åŒ¯å‡º `dataset_metadata.json`ï¼Œè¨˜éŒ„æ¯ç­†çš„æ ¼å¼ã€æª”æ¡ˆè·¯å¾‘èˆ‡éƒ¨åˆ†å…§å®¹ã€‚

---

## ä¸‰ã€æ¸¬è©¦ç¨‹å¼ç¢¼ & é©—è­‰é …ç›®

| æ¸¬è©¦æª”æ¡ˆ                                 | æ¸¬è©¦å…§å®¹                                           |
| ------------------------------------ | ---------------------------------------------- |
| `test_data_factory.py`               | é©—è­‰ Faker ç”¢ç”Ÿè³‡æ–™é›†åŠŸèƒ½                               |
| `pii_test_suite.py`                  | Regex/BERT/Composite/Replacer å–®å…ƒæ¸¬è©¦             |
| `test_detectors.py`                  | å¤š detector åƒæ•¸åŒ– correctness                     |
| `test_replacer.py`                   | ç›¸åŒåŸå§‹å­—ä¸²æ›¿æ›ä¸€è‡´æ€§                                    |
| `test_onnx_speed.py`                 | ONNX æ¨¡å¼å»¶é² < 25 ms                              |
| `integration_test.py`                | `extractâ†’detectâ†’replace` æ•´åˆæµç¨‹                  |
| `performance_test.py`                | ä¸åŒæ–‡æœ¬é•·åº¦ï¼ˆ1k/5k/10k/20kï¼‰æ•ˆèƒ½åŸºæº–                      |
| `end_to_end_test.py`                 | TXT/PDF/Image E2E æ¸¬è©¦ï¼Œæº–ç¢ºåº¦ â‰¥ 80%                 |
| `test_data_generator_integration.py` | å‡è³‡æ–™ç”Ÿæˆå™¨è¼¸å‡ºèˆ‡ `DeidPipeline` æ•´åˆï¼Œåµæ¸¬ç‡ â‰¥ 95%ã€ä¸€è‡´æ€§ 100% |

### æ¸¬è©¦ç›®çš„

1. **åŠŸèƒ½æ­£ç¢ºæ€§**ï¼šå„ Detectorã€Replacerã€Parser å–®å…ƒè¼¸å‡ºç¬¦åˆé æœŸã€‚
2. **æ•´åˆæµç¨‹**ï¼šPipeline å¾å„æ ¼å¼æŠ½å–ã€PII åµæ¸¬åˆ°æ›¿æ›ä¸æ¼è®€ã€ä¸ç ´å£æ ¼å¼ã€‚
3. **æ•ˆèƒ½åŸºæº–**ï¼šONNX vs PyTorch æ¨è«–é€Ÿç‡ï¼›ä¸åŒæ–‡æœ¬é•·åº¦å»¶é²ã€‚
4. **ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰**ï¼šå«å½±åƒ OCR â†’ åµæ¸¬ â†’ æ›¿æ›ï¼Œå…¨é¢é©—è­‰ã€‚
5. **ç”Ÿæˆå™¨é©—è­‰**ï¼šè‡ªå‹•ç”¢ç”Ÿçš„å‡è³‡æ–™ï¼Œå¿…é ˆèƒ½è¢« Pipeline åµæ¸¬ï¼Œä¸”æ›¿æ›ä¸€è‡´ã€‚

---

## å››ã€æ•æ„Ÿå‡è³‡æ–™ç”Ÿæˆèˆ‡å¾ŒçºŒæ‡‰ç”¨

1. **ç”Ÿæˆ**ï¼š

   * å‘¼å« `PIIGenerator` ç³»åˆ—æ–¹æ³•ç”¢ç”Ÿå–®ä¸€ PIIã€‚
   * é€é `DataFormatter`ï¼`AdvancedDataFormatter` æŠŠ PII åµŒå…¥å…¨æ–‡ä»¶æ–‡æœ¬æˆ–æ®µè½ã€‚
   * å†ç”± `AdvancedFileWriter`ï¼`FileWriter` è¼¸å‡ºå¤šç¨®æ ¼å¼æª”æ¡ˆã€‚

2. **æ‡‰ç”¨ç¯„ä¾‹**ï¼š

   * åœ¨ CI/CD ä¸­å…ˆè¡Œç”¢ç”Ÿ 100+ æª”æ¡ˆï¼Œæ”¾åˆ° `test_dataset/`ã€‚
   * è‡ªå‹•åŒ–æ¸¬è©¦ç®¡ç·š `run_automated_pipeline.py` â†’ é©—è­‰æ¯å€‹æª”æ¡ˆ PII åµæ¸¬ç‡ã€è™•ç†æ™‚é–“ã€‚
   * `benchmark_formats.py` â†’ æ¯”è¼ƒ PDFã€DOCXã€XLSXã€PNG å„è‡ªå¹³å‡/æœ€æ…¢/æœ€å¿«è™•ç†æ™‚é–“ã€‚
   * `validate_quality.py` â†’ é©—è­‰åŸå§‹ PIIs æ˜¯å¦å…¨è¢«ç§»é™¤ï¼Œä¸¦æª¢æŸ¥æ ¼å¼ä¿ç•™æƒ…æ³ã€‚

---

### PII åµæ¸¬å™¨æ¨¡çµ„èªªæ˜

#### `processor.py`
è·¯å¾‘ï¼š`src/deid_pipeline/image_deid/processor.py`
**åŠŸèƒ½å®šä½**
- é¡åˆ¥ï¼š`ImageDeidProcessor`
- è² è²¬ï¼šå°‡å½±åƒ OCR â†’ PII åµæ¸¬ â†’ æ›¿æ›ï¼é®è”½ â†’ å›å‚³å«åŸæ–‡ã€æ¸…ç†å¾Œæ–‡å­—ã€åµæ¸¬çµæœã€äº‹ä»¶èˆ‡è€—æ™‚

**å¯¦ä½œåŸç†**
1. ç”¨ OpenCV è®€æª”
2. é€é EasyOCR (singleton) æŠ½æ–‡å­— `(bbox, text, conf)`
3. åˆä½µæ–‡å­— â†’ `original_text`
4. å‘¼å«è¤‡åˆåµæ¸¬å™¨ `self.detector.detect(â€¦)`
5. ç”¨ `self.replacer.replace(â€¦)` å¥—ä¸Šå‡è³‡æ–™æˆ–é»‘æ¡†
6. å›å‚³æ‰€æœ‰ä¸­é–“çµæœèˆ‡è€—æ™‚

---

#### `ocr.py`
è·¯å¾‘ï¼š`src/deid_pipeline/parser/ocr.py`
**åŠŸèƒ½å®šä½**
- å‡½å¼ï¼š`get_ocr_reader(langs)`
- è² è²¬ï¼šå–®ä¾‹ç®¡ç† EasyOCR Readerï¼Œé è¨­è®€å– `Config.OCR_LANGUAGES`ï¼Œé¿å…é‡è¤‡åˆå§‹åŒ–

**å¯¦ä½œåŸç†**
```python
if _OCR_READER is None:
    _OCR_READER = easyocr.Reader(langs, gpu=False)
return _OCR_READER
````

* å–®ä¾‹æ¨¡å¼ç¯€çœæ¨¡å‹è¼‰å…¥æ™‚é–“
* èªè¨€æ¸…å–®ç”± Config æ§åˆ¶

---

#### `text_extractor.py`

è·¯å¾‘ï¼š`src/deid_pipeline/parser/text_extractor.py`
**åŠŸèƒ½å®šä½**

* å‡½å¼ï¼š`extract_text(file_path, ocr_fallback=True)`
* è² è²¬ï¼šå¾å¤šç¨®æ ¼å¼ï¼ˆ`.txt`ã€`.docx`ã€`.html`ã€`.pdf`ï¼‰æå–æ–‡å­—ä¸¦å›å‚³ offset map

**å¯¦ä½œåŸç†**

1. æ–‡å­—ï¼Wordï¼HTML â†’ ç›´è®€å…¨æ–‡ + å»ºç«‹ charâ†’(page, bbox) map
2. PDF â†’ ç”¨ `fitz` æŠ½ blocksï¼Œè‹¥æ–‡å­—éå°‘(`len<Config.OCR_THRESHOLD`) â†’ OCR fallback
3. å›å‚³ `(full_text, offset_map)`

---

### PII åµæ¸¬å™¨ç³»åˆ—

#### `spacy_detector.py`

è·¯å¾‘ï¼š`src/deid_pipeline/pii/detectors/legacy/spacy_detector.py`
**åŠŸèƒ½å®šä½**

* SpaCy NER + Regex é›™åˆ€æµ

**å¯¦ä½œåŸç†**

1. `nlp = spacy.load(...)` â†’ `doc.ents`
2. ç¯©é¸ `SPACY_TO_PII_TYPE`
3. `Entity(..., score=0.99, source="spacy")`
4. åŠ å…¥ `Config.REGEX_PATTERNS` æ­£å‰‡åŒ¹é… results
5. `_resolve_conflicts(...)` ä¿ç•™æœ€é«˜åˆ†æˆ–å„ªå…ˆç´š

---

#### `regex_detector.py`

è·¯å¾‘ï¼š`src/deid_pipeline/pii/detectors/regex_detector.py`
**åŠŸèƒ½å®šä½**

* å–®ç´”ç”¨æ­£å‰‡ `re.finditer` æƒ PII

**å¯¦ä½œåŸç†**

```python
for type, patterns in Config.REGEX_PATTERNS.items():
    for pat in patterns:
        for m in re.compile(pat).finditer(text):
            yield Entity(span=(m.start(), m.end()), type=type, score=1.0, source="regex")
```

---

#### `bert_detector.py`

è·¯å¾‘ï¼š`src/deid_pipeline/pii/detectors/bert_detector.py`
**åŠŸèƒ½å®šä½**

* PyTorch Transformers BERT Token Classification

**å¯¦ä½œåŸç†**

1. `__init__`è¼‰å…¥ ONNX æˆ– PyTorch æ¨¡å‹ + tokenizer
2. `detect(text)` â†’ sliding window åˆ‡å¡Š
3. æ¯æ®µåšæ¨è«– â†’ å›å‚³ token label
4. `_merge_entities(...)` å»é‡åˆã€ä¾ `ENTITY_PRIORITY` ä¿ç•™

---

#### `bert_onnx_detector.py`

è·¯å¾‘ï¼š`src/deid_pipeline/pii/detectors/bert_onnx_detector.py`
**åŠŸèƒ½å®šä½**

* ONNX Runtime åŠ é€Ÿç‰ˆ BERT åµæ¸¬

**å·®ç•°**

* æ¨¡å‹è¼‰å…¥æ”¹ç”¨ `ORTModelForTokenClassification.from_pretrained(...)`
* æ¨è«–æ”¹æˆ `session.run(...)`

---

#### `composite.py`

è·¯å¾‘ï¼š`src/deid_pipeline/pii/detectors/composite.py`
**åŠŸèƒ½å®šä½**

* å°‡å‰è¿°æ‰€æœ‰åµæ¸¬å™¨çµæœã€Œparallel åŸ·è¡Œ â†’ åˆä½µå»é‡ã€

**å¯¦ä½œåŸç†**

```python
all_ents = []
for det in self.detectors:
    all_ents.extend(det.detect(text))
return self._resolve_conflicts(all_ents)
```

* ä¾ `ENTITY_PRIORITY` èˆ‡ score æ±ºå®šæœ€çµ‚ä¿ç•™

### åµæ¸¬å™¨èˆ‡å·¥å…·æ¨¡çµ„èªªæ˜

#### `regex_detector.py`
è·¯å¾‘ï¼š`src/deid_pipeline/pii/detectors/regex_detector.py`
- **åŠŸèƒ½**ï¼šå‹•æ…‹è¼‰å…¥ `configs/regex_zh.yaml` ä¸­çš„å¤šå€‹æ­£å‰‡è¦å‰‡ï¼Œå°æ–‡å­—åšå…¨æ–‡æƒæï¼Œå›å‚³æ‰€æœ‰å‘½ä¸­çš„ PII Entity
- **å¯¦ä½œè¦é»**ï¼š
  1. `load_rules()` ç”¨ `os.path.getmtime` æª¢æŸ¥æª”æ¡ˆæ›´æ–°ä¸¦é‡è¼‰
  2. æ”¯æ´ `"IGNORECASE|MULTILINE"` ç­‰å¤š flag å­—ä¸²è§£æ
  3. `detect(text)` â†’ `for (type,pattern) in rules: pattern.finditer(text)` â†’ `Entity(span, type, score=1.0, source="regex")`

---

#### `__init__.py` (detectors)
è·¯å¾‘ï¼š`src/deid_pipeline/pii/detectors/__init__.py`
- **åŠŸèƒ½**ï¼šé›†ä¸­å¼•å…¥å„ Detector ä¸¦å¯¦ä½œ `get_detector(lang)`
- **é¸æ“‡é‚è¼¯**ï¼š
  1. æ ¹æ“šèªè¨€ (`zh`/`en`)
  2. `Config.USE_STUB` é–‹é—œ
  3. è‹¥å•Ÿç”¨ ONNXï¼Œä¸”æ¨¡å‹å­˜åœ¨ â†’ å›å‚³ ONNX + Regex
  4. å¦å‰‡å›å‚³ PyTorch BERT + Regex
  5. `CompositeDetector` è² è²¬å¤šæª¢æ¸¬å™¨åˆä½µèˆ‡å»è¡çª

---

#### `config.py`
è·¯å¾‘ï¼š`src/deid_pipeline/config.py`
- **åŠŸèƒ½**ï¼šå…¨åŸŸè¨­å®šä¸­å¿ƒ
- **ä¸»è¦è¨­å®š**ï¼š
  - Regex è¦å‰‡æª”è·¯å¾‘ã€`OCR_LANGUAGES`ã€`OCR_THRESHOLD`
  - BERTï¼š`NER_MODEL_PATH`, `MAX_SEQ_LENGTH`, `WINDOW_STRIDE`, `ENTITY_PRIORITY`
  - ONNXï¼š`USE_ONNX`, `ONNX_MODEL_PATH`, `ONNX_PROVIDERS`
  - Fake-dataï¼š`GPT2_MODEL_PATH`, `FAKER_LOCALE`
  - ç®¡ç·šæ——æ¨™ï¼š`USE_STUB`, `ENABLE_PROFILING`, `LOG_LEVEL`

---

#### `fake_provider.py`
è·¯å¾‘ï¼š`src/deid_pipeline/pii/utils/fake_provider.py`
- **åŠŸèƒ½**ï¼šæ··åˆ GPT-2 + Faker çš„ PII å‡è³‡æ–™ç”¢ç”Ÿ
- **å¯¦ä½œè¦é»**ï¼š
  1. `GPT2Provider.generate(prompt)` â†’ å¤±æ•—å‰‡
  2. `Faker("zh_TW")` fallback
  3. å…§éƒ¨ cache é¿å…é‡è¤‡ç”ŸæˆåŒä¸€åŸå§‹å­—ä¸²

---

#### `replacer.py`
è·¯å¾‘ï¼š`src/deid_pipeline/pii/utils/replacer.py`
- **åŠŸèƒ½**ï¼šæ ¹æ“š `Entity.span` æœ‰åºæ›¿æ›æˆ–å›å‚³é®é»‘åº§æ¨™
- **å¯¦ä½œè¦é»**ï¼š
  1. `entities` å…ˆæŒ‰ `start` æ’åº
  2. æ»‘å‹•æ‹¼æ¥æ–°å­—ä¸²ï¼Œæ›´æ–° `offset`
  3. æ”¯æ´ `"replace"` èˆ‡ `"black"` æ¨¡å¼
  4. `dumps(events)` â†’ JSON

---

#### æª”æ¡ˆä¸²æ¥

åœ¨ `src/deid_pipeline/pii/detectors/__init__.py` ä¸­ï¼š

```python
from .spacy_detector import SpacyDetector
from .regex_detector import RegexDetector
from .bert_detector import BertNERDetector
from .bert_onnx_detector import BertONNXNERDetector
from .composite import CompositeDetector

def get_detector(lang="zh"):
    # æ ¹æ“š Config.USE_ONNX / USE_STUB çµ„æˆ CompositeDetector(...)
    return CompositeDetector(...)
```

---


### ğŸ” sensitive_data_generator

é€™å€‹å­æ¨¡çµ„è² è²¬ã€Œåˆæˆã€å¤šæ ¼å¼ã€å«æ•æ„Ÿè³‡æ–™çš„å‡æ¸¬è©¦æ–‡ä»¶ï¼Œä¾› De-ID pipeline æ¸¬è©¦èˆ‡ benchmarkã€‚

#### 2.1 `__init__.py`

```python
from .config import *
from .generators import PIIGenerator
from .formatters import DataFormatter
from .advanced_formatters import AdvancedDataFormatter
from .file_writers import FileWriter
from .advanced_file_writers import AdvancedFileWriter
from .dataset_generator import MultiFormatDatasetGenerator

__all__ = [
  "PIIGenerator", "DataFormatter", "FileWriter",
  "AdvancedDataFormatter","AdvancedFileWriter","MultiFormatDatasetGenerator"
]
````

* **åŠŸèƒ½**ï¼šæŠŠæ¨¡çµ„è£¡çš„æ ¸å¿ƒé¡åˆ¥ä¸€æ¬¡å°å‡º (`__all__`)ï¼Œæä¾›ä¸Šå±¤ `import sensitive_data_generator` å°±èƒ½æ‹¿åˆ°ç”¢ç”Ÿå™¨ã€æ ¼å¼å™¨ã€æª”æ¡ˆè¼¸å‡ºç­‰æ‰€æœ‰å·¥å…·ã€‚

#### 2.2 `advanced_file_writers.py`

```python
class AdvancedFileWriter:
    """é€²éšå¤šæ ¼å¼æª”æ¡ˆè¼¸å‡ºå·¥å…·"""

    @staticmethod
    def create_complex_pdf(content, output_dir, filename=None, include_charts=True):
        # 1. ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        # 2. å»ºç«‹ ReportLab PDF æ–‡ä»¶
        doc = SimpleDocTemplate(filepath, pagesize=letter)
        styles = getSampleStyleSheet()
        elements = []

        # 3. åŠ æ¨™é¡Œèˆ‡æ­£æ–‡
        title = Paragraph("æ©Ÿå¯†æ–‡ä»¶ â€“ å€‹äººè³‡æ–™å ±å‘Š", styles['Heading1'])
        elements.append(title)
        elements.append(Spacer(1, 12))
        pii_para = Paragraph(content, styles['BodyText'])
        elements.append(pii_para)
        elements.append(Spacer(1, 12))

        # 4. åŠ è¡¨æ ¼ï¼ˆç¤ºç¯„æ’å…¥ 4 æ¬„ï¼šå§“åã€IDã€é›»è©±ã€åœ°å€ï¼‰
        table_data = [
          ['é …ç›®','åŸå§‹è³‡æ–™','å‚™è¨»'],
          ['å§“å', PIIGenerator.generate_tw_name(), 'æ¸¬è©¦ç”¨è™›æ“¬å§“å'],
          ['èº«åˆ†è­‰', PIIGenerator.generate_tw_id(), 'æ¸¬è©¦ç”¨è™›æ“¬ID'],
          ['é›»è©±', PIIGenerator.generate_tw_phone(), 'æ¸¬è©¦ç”¨è™›æ“¬é›»è©±'],
          ['åœ°å€', PIIGenerator.generate_tw_address(), 'æ¸¬è©¦ç”¨è™›æ“¬åœ°å€']
        ]
        table = Table(table_data, colWidths=[1.5*inch,3*inch,2.5*inch])
        table.setStyle(TableStyle([...]))
        elements.append(table)
        elements.append(Spacer(1, 24))

        # 5. å¯é¸ï¼šæ’å…¥å‡åœ–è¡¨ï¼Œåœ–ç”¨ PIL+matplotlib ç”Ÿæˆ
        if include_charts:
            chart_img = AdvancedFileWriter.generate_fake_chart()
            elements.append(RLImage(chart_img, width=5*inch, height=3*inch))
            elements.append(Paragraph("åœ–1ï¼šæ¸¬è©¦è³‡æ–™åˆ†ä½ˆåœ–", styles['Italic']))

        # 6. å¯«å‡º PDF
        doc.build(elements)
        return filepath
```

* **åŠŸèƒ½æ‹†è§£**

  1. **ç›®éŒ„æª¢æŸ¥**ï¼š`os.makedirs(...)`
  2. **PDF**ï¼šä½¿ç”¨ ReportLab `SimpleDocTemplate` + `Paragraph`ï¼‹`Table`ï¼‹`Spacer`
  3. **å‡è³‡æ–™è¡¨æ ¼**ï¼š`PIIGenerator` éš¨æ©Ÿç”Ÿæˆå§“åã€IDã€é›»è©±ã€åœ°å€
  4. **å‡åœ–è¡¨**ï¼šå‘¼å« `generate_fake_chart()` â†’ éš¨æ©Ÿç”¢ç”Ÿ bar/line/pie åœ–
  5. **åŒ¯å‡º**ï¼šå›å‚³å®Œæ•´æª”æ¡ˆè·¯å¾‘

```python
    @staticmethod
    def generate_fake_chart():
        """ç”Ÿæˆ Bar/Line/Pie å‡åœ–è¡¨"""
        plt.figure(figsize=(8,5))
        kind = random.choice(['bar','line','pie'])
        if kind=='bar':
            labels = ['Aéƒ¨é–€','Béƒ¨é–€','Céƒ¨é–€','Déƒ¨é–€']
            values = np.random.randint(100,500,size=4)
            plt.bar(labels, values)
            plt.title('éƒ¨é–€æ¥­ç¸¾æ¯”è¼ƒ')
        elif kind=='line':
            x = np.arange(1,11)
            y = np.random.rand(10)*100
            plt.plot(x,y,marker='o')
            plt.title('æœˆåº¦è¶¨å‹¢åˆ†æ')
        else:
            labels = ['é¡åˆ¥A','é¡åˆ¥B','é¡åˆ¥C','é¡åˆ¥D']
            sizes = np.random.randint(15,40,size=4)
            plt.pie(sizes, labels=labels, autopct='%1.1f%%')
            plt.title('é¡åˆ¥åˆ†ä½ˆåœ–')
        buf = io.BytesIO()
        plt.tight_layout()
        plt.savefig(buf, format='png', dpi=100)
        plt.close()
        buf.seek(0)
        return buf
```

* **åŠŸèƒ½**ï¼šç”¨ matplotlib éš¨æ©Ÿé¸æ“‡åœ–è¡¨é¡å‹ã€ç”Ÿæˆæ•¸æ“šå¾Œè¼¸å‡ºåˆ° `BytesIO`ï¼Œè®“ä¸Šå±¤ PDF/Word/PPTX éƒ½å¯ä»¥ç›´æ¥æ’åœ–ã€‚

> **å¾ŒçºŒ**ï¼š`create_word_document`ã€`create_powerpoint_presentation`ã€`create_excel_spreadsheet`ã€`create_scanned_document` éƒ½æ¡ç›¸åŒæ‹†åˆ†ï¼š
>
> * **Word** â†’ `python-docx`ï¼š`Document()`ã€`add_heading`ã€`add_table`ã€`add_picture`
> * **PPTX** â†’ `python-pptx`ï¼š`Presentation()`ã€`slides.add_slide()`ã€`shapes.add_table()`ã€`shapes.add_picture()`
> * **Excel** â†’ `pandas.DataFrame` + `ExcelWriter(engine='xlsxwriter')`ï¼›è¨­å®š header æ ¼å¼ã€æ¬„å¯¬ã€æ•¸å€¼æ ¼å¼
> * **æƒææª”** â†’ `PIL.ImageDraw`ï¼šç•«èƒŒæ™¯å™ªé»ã€æ–‡å­—ã€ç°½ç« ã€ç°½åï¼Œæ¨¡æ“¬æƒæå“è³ª

ä¸‹é¢ç¤ºç¯„å¦‚ä½•æŠŠ **`advanced_formatters.py`**ã€**`config.py`**ã€**`dataset_generator.py`** ä¹ŸåŒæ¨£è£œåˆ°æ–‡ä»¶è£¡ï¼Œä¸¦èªªæ˜æ¯å€‹å€å¡Šçš„åŠŸèƒ½èˆ‡ç›®çš„ã€‚


#### 2.3 `advanced_formatters.py`

```python
class AdvancedDataFormatter:
    """é€²éšè³‡æ–™æ ¼å¼åŒ–ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_contract_document():
        """
        ç”¢ç”Ÿä¸€ä»½åˆç´„åˆç´„ç¯„æœ¬ï¼ˆå«è™›æ“¬ç•¶äº‹äººè³‡æ–™ï¼‰ï¼š
        - parties: éš¨æ©Ÿç”¢ç”Ÿç”²ä¹™é›™æ–¹å§“åã€èº«åˆ†è­‰ã€åœ°å€ã€ç°½è¨‚æ—¥æœŸ
        - contract: å¡«å…¥å„æ¢æ¬¾æ¨£æ¿ï¼ˆç›®çš„ã€æœŸé™ã€å ±é…¬ã€ä¿å¯†ã€ç®¡è½„æ³•é™¢ç­‰ï¼‰
        """
        parties = {
          "ç”²æ–¹": PIIGenerator.generate_tw_name(),
          "ä¹™æ–¹": PIIGenerator.generate_tw_name(),
          "ç”²æ–¹èº«åˆ†è­‰": PIIGenerator.generate_tw_id(),
          "ä¹™æ–¹èº«åˆ†è­‰": PIIGenerator.generate_tw_id(),
          "ç”²æ–¹åœ°å€": PIIGenerator.generate_tw_address(),
          "ä¹™æ–¹åœ°å€": PIIGenerator.generate_tw_address(),
          "ç°½ç´„æ—¥æœŸ": (datetime.now() - timedelta(days=random.randint(1,365)))\
             .strftime("%Yå¹´%mæœˆ%dæ—¥")
        }
        contract = f"""
        åˆç´„æ›¸

        ç«‹åˆç´„ç•¶äº‹äººï¼š
        ç”²æ–¹ï¼š{parties['ç”²æ–¹']}ï¼ˆèº«åˆ†è­‰è™Ÿï¼š{parties['ç”²æ–¹èº«åˆ†è­‰']}ï¼‰
        ...
        ç¬¬å…­æ¢ ç®¡è½„æ³•é™¢
        å› æœ¬åˆç´„ç™¼ç”Ÿä¹‹çˆ­è­°ï¼Œé›™æ–¹åŒæ„ä»¥å°ç£å°åŒ—åœ°æ–¹æ³•é™¢ç‚ºç¬¬ä¸€å¯©ç®¡è½„æ³•é™¢ã€‚

        ä¸­è¯æ°‘åœ‹ {parties['ç°½ç´„æ—¥æœŸ']}
        """
        return contract
````

* **åŠŸèƒ½**ï¼šç”¨ `PIIGenerator` éš¨æ©Ÿå¡«å…¥ã€Œåˆç´„ã€æ‰€éœ€é—œéµæ¬„ä½ï¼Œä¸¦é€éå¤šè¡Œå­—ä¸²æ¨¡æ¿ï¼ˆf-stringï¼‰çµ„æˆå®Œæ•´åˆç´„ç¯„æœ¬ã€‚

```python
    @staticmethod
    def generate_medical_report():
        """
        ç”Ÿæˆè©³ç´°é†«ç™‚å ±å‘Šæ–‡æœ¬ï¼ˆå«è™›æ“¬ç—…äººè³‡æ–™ + è™›æ“¬æª¢æŸ¥æ•¸æ“šï¼‰ï¼š
        - patient: éš¨æ©Ÿå§“åã€IDã€å‡ºç”Ÿã€é›»è©±ã€åœ°å€ã€ç—…æ­·è™Ÿ
        - test_results: è¡€å£“ã€å¿ƒç‡ã€è¡€ç³–ã€è†½å›ºé†‡ç­‰
        - report: f-string å¡«å…¥é†«é™¢åç¨±ã€å„ç¯€æ¨™é¡Œï¼ˆç—…å²ã€è¨ºæ–·ã€æª¢é©—ã€å½±åƒã€è™•æ–¹ã€é†«å›‘ï¼‰
        """
```

* **åŠŸèƒ½**ï¼šåŒæ¨£ç”¨ f-string + `HOSPITALS` åˆ—è¡¨éš¨æ©ŸæŒ‘é¸é†«é™¢ï¼Œçµ„å‡ºå¯ç›´æ¥è²¼æª”çš„é†«ç™‚å ±å‘Šæ¨¡æ¿ã€‚

---

#### 2.4 `config.py`

```python
# å°ç£åœ°å€å¸¸ç”¨åƒè€ƒè³‡æ–™ï¼Œä¾› Formatter/Generator ä½¿ç”¨
TAIWAN_LOCATIONS = {
  "åŒ—éƒ¨": ["å°åŒ—å¸‚","æ–°åŒ—å¸‚","åŸºéš†å¸‚",...],
  "ä¸­éƒ¨": ["å°ä¸­å¸‚","å½°åŒ–ç¸£",...],
  ...
}

STREET_NAMES = ["ä¸­å±±","ä¸­æ­£","å…‰å¾©",...]
SURNAMES     = ["é™³","æ—","å¼µ",...]
GIVEN_NAMES  = ["æ€¡å›","å¿—æ˜","é›…å©·",...]
HOSPITALS    = ["å°å¤§é†«é™¢","é•·åºšç´€å¿µé†«é™¢",...]
MEDICAL_SPECIALTIES = ["å…§ç§‘","å¤–ç§‘","å…’ç§‘",...]
```

* **åŠŸèƒ½**ï¼šæŠŠæ‰€æœ‰å¯éš¨æ©Ÿé¸ç”¨çš„åœ°åã€è¡—é“ã€å§“åã€é†«é™¢ã€ç§‘åˆ¥ç­‰åˆ—è¡¨é›†ä¸­ç®¡ç†ï¼Œæ–¹ä¾¿ Formatter å‘¼å«ã€‚

---

#### 2.5 `dataset_generator.py`

```python
class MultiFormatDatasetGenerator:
    """å¤šæ ¼å¼æ•æ„Ÿè³‡æ–™é›†ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_full_dataset(output_dir, num_items=50):
        """
        ä¸€æ¬¡ç”Ÿç”¢å¤šç¨®æ ¼å¼ï¼ˆpdfã€wordã€imageã€excelã€pptã€contractsã€medicalã€financialâ€¦ï¼‰
        - å»ºç«‹å­è³‡æ–™å¤¾ï¼špdf/ã€word/ã€scanned/ã€excel/ã€ppt/ã€contracts/ã€medical/ã€financial/
        - é€ç­†å¾ªç’°ï¼šéš¨æ©Ÿé¸ contract/medical/financialï¼Œå‘¼å« AdvancedDataFormatter ç”¢æ–‡æœ¬
        - å‘¼å« AdvancedFileWriter è¼¸å‡ºå°æ‡‰æ ¼å¼æª”æ¡ˆä¸¦ç´€éŒ„è·¯å¾‘
        - æœ€å¾ŒåŒ¯å‡º metadata.jsonï¼ŒåŒ…å«æ¯ç­†çš„æ ¼å¼æ¸…å–®èˆ‡æª”æ¡ˆä½ç½®
        """
        # å»ºç›®éŒ„ã€åˆå§‹åŒ– dataset listâ€¦
        sub_dirs = {â€¦}
        for i in range(num_items):
          doc_type = random.choice(["contract","medical","financial"])
          if doc_type=="contract":
            content = AdvancedDataFormatter.generate_contract_document()
          elif doc_type=="medical":
            content = AdvancedDataFormatter.generate_medical_report()
          else:
            content = AdvancedDataFormatter.generate_financial_statement()

          pdf_path = AdvancedFileWriter.create_complex_pdf(content, sub_dirs["pdf"], f"{doc_type}_{i+1}.pdf")
          item["formats"].append({"format":"pdf","path":pdf_path})

          # â€¦åŒç†å‘¼å« create_word_documentã€create_scanned_document
          # è‹¥ financial é¡å¤–å‘¼å« create_excel_spreadsheetã€create_powerpoint_presentation

          # å¯« content .txtã€dataset.append(item)
        # å¯«å‡º dataset_metadata.json
```

* **åŠŸèƒ½**ï¼šæ•´åˆä»¥ä¸Š Formatter + FileWriterï¼Œæ‰¹æ¬¡ç”Ÿç”¢å¤šæ ¼å¼æ¸¬è©¦é›†ä¸¦è¼¸å‡º metadataï¼Œä¾¿æ–¼å¾ŒçºŒè‡ªå‹•åŒ–æ¸¬è©¦èˆ‡ benchmarkã€‚

ä¸‹é¢ç¤ºç¯„å¦‚ä½•æŠŠ **`file_writers.py`**ã€**`formatters.py`**ã€**`generators.py`** ä¹ŸåŠ å…¥èªªæ˜ï¼Œæµç¨‹èˆ‡å…ˆå‰ä¸€è‡´ï¼š

#### 2.6 `file_writers.py`

```python
class FileWriter:
    """æª”æ¡ˆè¼¸å‡ºå·¥å…·"""

    @staticmethod
    def write_text_file(content, output_dir, filename=None):
        """
        å°‡æ–‡å­—å…§å®¹å¯«å…¥ .txt æª”
        - è‡ªå‹•å»ºç«‹è³‡æ–™å¤¾
        - è‹¥æœªæŒ‡å®š filenameï¼Œå‰‡ç”¨ timestamp å‘½å
        - å›å‚³æª”æ¡ˆå®Œæ•´è·¯å¾‘
        """
        ...

    @staticmethod
    def write_pdf_file(content, output_dir, filename=None):
        """
        å°‡æ–‡å­—å…§å®¹å¯«å…¥ PDF
        - ä½¿ç”¨ fpdf å¥—ä»¶
        - æ”¯æ´å¤šè¡Œæ–‡å­—æ’ç‰ˆï¼ˆmulti_cellï¼‰
        - å›å‚³æª”æ¡ˆå®Œæ•´è·¯å¾‘
        """
        ...

    @staticmethod
    def write_csv_file(rows, output_dir, filename=None):
        """
        å°‡ list-of-dict å¯«æˆ CSV
        - è‡ªå‹•å»ºç«‹è³‡æ–™å¤¾
        - ä¾ dict keys ä½œç‚ºæ¬„ä½
        """
        ...
```

* **ç›®çš„**ï¼šæä¾›æœ€åŸºæœ¬çš„ã€Œæ–‡å­— / PDF / CSVã€æª”æ¡ˆè¼¸å‡ºèƒ½åŠ›ï¼Œä¾›ä¸Šå±¤ generator è¼•é¬†å‘¼å«ã€‚

#### 2.7 `formatters.py`

```python
class DataFormatter:
    """æ•æ„Ÿè³‡æ–™æ®µè½ & æ–‡ä»¶ç¯„æœ¬ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_paragraph(min_sentences=3, max_sentences=8, pii_density=0.3):
        """
        ç”¨å¤šç¨®å¥å‹ç¯„æœ¬éš¨æ©Ÿæ‹¼å‡ºä¸€æ®µæ–‡å­—ï¼Œä¸¦ä¾ç…§ pii_density æ’å…¥ PII
        - sentence_templates: å¤šç¨®å«ä½”ä½ç¬¦ {NAME}/{PHONE}/{ADDRESS}â€¦ çš„å¥å­
        - éš¨æ©Ÿæ±ºå®šè¦æ’å¹¾å¥ã€æ¯å¥æ˜¯å¦è¦æ›¿æ›æˆ PII
        """
        ...

    @staticmethod
    def generate_medical_record():
        """
        ç”Ÿæˆå®Œæ•´é†«ç™‚ç´€éŒ„å­—ä¸²
        - åŸºæœ¬è³‡è¨Šï¼ˆå§“å/æ€§åˆ¥/å‡ºç”Ÿ/èº«åˆ†è­‰/é›»è©±/åœ°å€/ç—…æ­·è™Ÿï¼‰
        - å°±è¨ºè³‡è¨Šï¼ˆæ—¥æœŸ/é†«é™¢/ç§‘åˆ¥/é†«å¸«ï¼‰
        - è¨ºæ–·èˆ‡è™•æ–¹ã€ç”¨è—¥å»ºè­°
        """
        ...

    @staticmethod
    def generate_financial_document():
        """
        ç”Ÿæˆè²¡å‹™å ±è¡¨æ–‡å­—
        - å®¢æˆ¶åŸºæœ¬è³‡æ–™ï¼ˆå§“å/ID/è¯çµ¡/å¸³è™Ÿ/ä¿¡ç”¨å¡ï¼‰
        - éš¨æ©Ÿ 3ï½10 ç­†äº¤æ˜“è¨˜éŒ„
        - è¨ˆç®—ç¸½é¤˜é¡ã€æ”¯å‡ºçµ±è¨ˆ
        """
```

* **ç›®çš„**ï¼šå°‡åŸå§‹ PII ç”Ÿæˆå™¨ï¼ˆ`PIIGenerator`ï¼‰è½‰æˆå¯è²¼æ–‡ä»¶çš„è‡ªç„¶æ®µè½æˆ–å®Œæ•´æ–‡ä»¶ç¯„æœ¬ã€‚

#### 2.8 `generators.py`

```python
class PIIGenerator:
    """ç¹é«”ä¸­æ–‡å„é¡ PII éš¨æ©Ÿç”Ÿæˆå™¨"""

    @staticmethod
    def generate_tw_id():
        """ç¬¦åˆè¦å‰‡çš„è‡ºç£èº«åˆ†è­‰å­—è™Ÿï¼ˆå«æª¢æ ¸ç¢¼ï¼‰"""
        ...

    @staticmethod
    def generate_tw_phone():
        """è‡ºç£æ‰‹æ©Ÿè™Ÿç¢¼ï¼ˆ0912-345-678 æˆ– 0912345678ï¼‰"""
        ...

    @staticmethod
    def generate_tw_address():
        """è‡ºç£åœ°å€ï¼šéš¨æ©Ÿå€åŸŸ + éš¨æ©Ÿè¡—é“ + é–€ç‰Œ + æ¨“å±¤"""
        ...

    @staticmethod
    def generate_tw_name():
        """éš¨æ©ŸæŒ‘é¸å¸¸è¦‹å§“æ° + åå­—ï¼ˆæœ‰ 30% æ©Ÿç‡é›™åï¼‰"""
        ...

    @staticmethod
    def generate_medical_record():
        """åƒ…å›å‚³ã€Œç—…æ­·è™Ÿã€æ ¼å¼ï¼Œä¾›ç¯„æœ¬æ’å…¥"""
        ...

    @staticmethod
    def generate_credit_card():
        """æ¨¡æ“¬ä¿¡ç”¨å¡å¡è™Ÿï¼ˆ16 ç¢¼ï¼‰"""
        ...

    ...ï¼ˆå…¶ä»–å¦‚ emailã€passportã€license_plateã€health_insuranceã€random_pii ç­‰ï¼‰...
```

* **ç›®çš„**ï¼šä½éš PII APIï¼Œå°ˆæ³¨ã€Œç”¢ç”Ÿä¸€å‰‡ã€å„ç¨®æ•æ„Ÿæ¬„ä½å€¼ï¼Œæ‰€æœ‰ä¸Šå±¤ Formatter / FileWriter / DatasetGenerator éƒ½å»ºæ§‹åœ¨å®ƒä¹‹ä¸Šã€‚

---

### ğŸ› ï¸ Scripts utilities

### 1. `benchmark_formats.py` â€” æ ¼å¼æ•ˆèƒ½åŸºæº–æ¸¬è©¦
```python
from deid_pipeline import DeidPipeline
def benchmark_formats(dataset_dir, formats=["pdf","docx","xlsx","png"]):
    pipeline = DeidPipeline(language="zh")
    for fmt in formats:
        fmt_files = [f for f in os.listdir(dataset_dir) if f.endswith(fmt)]
        # æ¯ç¨®æ ¼å¼åªæ¸¬ 10 å€‹æª”æ¡ˆ
        for file in fmt_files[:10]:
            start = time.time()
            pipeline.process(os.path.join(dataset_dir, file))
            processing_times.append(time.time()-start)
```

* **åŠŸèƒ½**ï¼šå°æŒ‡å®šè³‡æ–™å¤¾ä¸­ï¼Œå„æ ¼å¼å‰10å€‹æª”æ¡ˆåšå»è­˜åˆ¥åŒ–ï¼Œæ”¶é›†åŸ·è¡Œæ™‚é–“ã€‚
* **ç”¨é€”**ï¼šé‡åŒ–ä¸åŒæª”æ¡ˆæ ¼å¼ï¼ˆPDFã€Wordã€Excelã€PNGï¼‰åœ¨å»è­˜åˆ¥åŒ–æµç¨‹ä¸­çš„å¹³å‡ï¼æœ€å°ï¼æœ€å¤§è™•ç†æ™‚é–“ï¼Œå¹«åŠ©èª¿å„ªèˆ‡è³‡æºè¦åŠƒã€‚

---

### 2. `download_models.py` â€” æ¨¡å‹é ä¸‹è¼‰

```python
MODELS = {
  "ner_zh": ("ckiplab/bert-base-chinese-ner", "models/ner/bert-ner-zh"),
  "gpt2_base": ("gpt2", "models/gpt2")
}
for name, (repo_id, target) in MODELS.items():
    # Transformers ä¸‹è¼‰ GPT-2
    if name=="gpt2_base" and not (Path(target)/"pytorch_model.bin").exists():
        tokenizer = AutoTokenizer.from_pretrained(repo_id)
        model = AutoModelForCausalLM.from_pretrained(repo_id)
        model.save_pretrained(target); tokenizer.save_pretrained(target)
    # HF Hub snapshot ä¸‹è¼‰ NER
    elif not Path(target).exists():
        snapshot_download(repo_id, local_dir=target)
```

* **åŠŸèƒ½**ï¼šè‡ªå‹•å¾ HuggingFace åŠ Transformers ä¸‹è¼‰ã€å¿«ç…§ä¿å­˜ BERT-NER èˆ‡ GPT-2 æ¨¡å‹åˆ° `models/`ã€‚
* **ç”¨é€”**ï¼šç¢ºä¿åœ˜éšŠä¸€éµåŸ·è¡Œæ™‚å·²å…·å‚™æœ¬åœ°æ¨¡å‹ï¼Œé¿å…é¦–æ¬¡é‹è¡Œæ™‚æ‰‹å‹•ä¸‹è¼‰å¤±æ•—ã€‚

---

### 3. `run_automated_pipeline.py` â€” è‡ªå‹•åŒ–æ¸¬è©¦ç®¡ç·š

```python
from deid_pipeline import DeidPipeline
def run_automated_test_pipeline(dataset_dir):
    pipeline = DeidPipeline(language="zh")
    for root, _, files in os.walk(dataset_dir):
        for fn in files:
            res = pipeline.process(os.path.join(root, fn))
            results.append({
                "file": fn,
                "format": fn.split(".")[-1],
                "pii_count": len(res.entities),
                "processing_time": res.processing_time
            })
    json.dump(results, open("pipeline_results.json","w"), ensure_ascii=False, indent=2)
```

* **åŠŸèƒ½**ï¼šéè¿´éæ­·è³‡æ–™é›†è³‡æ–™å¤¾ï¼Œå°æ¯æ”¯æª”æ¡ˆå‘¼å« `DeidPipeline.process()`ï¼Œä¸¦æŠŠ PII åµæ¸¬æ•¸ã€åŸ·è¡Œæ™‚é–“è¼¸å‡ºæˆ JSONã€‚
* **ç”¨é€”**ï¼šå¿«é€Ÿæª¢è¦–æ•´æ‰¹æ¸¬è©¦è³‡æ–™çš„å»è­˜åˆ¥åŒ–æˆæ•ˆï¼Œæ–¹ä¾¿ç”Ÿæˆå ±è¡¨æˆ–ä¸Šå‚³ CIã€‚

---

### 4. `validate_quality.py` â€” å»è­˜åˆ¥åŒ–å“è³ªé©—è­‰

```python
def validate_deidentification_quality(original_dir, processed_dir):
    for orig in os.listdir(original_dir):
        proc = os.path.join(processed_dir, orig)
        orig_text = open(os.path.join(original_dir,orig)).read()
        proc_text = open(proc).read()
        # æª¢æŸ¥æ˜¯å¦ç§»é™¤æ‰€æœ‰ PII
        for label in ["èº«åˆ†è­‰","é›»è©±","åœ°å€","ç—…æ­·è™Ÿ"]:
            if label in orig_text and label in proc_text:
                pii_removed=False
        quality_report.append({...})
    # è¨ˆç®—æˆåŠŸç‡
    pii_success = sum(r["pii_removed"] for r in quality_report)/len(quality_report)
    print(f"PII Removal Success: {pii_success:.2%}")
```

* **åŠŸèƒ½**ï¼šé€ä¸€æ¯”å°åŸæª”èˆ‡è™•ç†å¾Œæª”ï¼Œé©—è­‰ã€Œæ‰€æœ‰æ¨™è¨»çš„ PIIã€ç¢ºå¯¦æœªå‡ºç¾åœ¨å»è­˜åˆ¥åŒ–çµæœä¸­ï¼ŒåŒæ™‚å¯ç•™å¾…æ“´å……ã€Œè¡¨æ ¼ã€åœ–è¡¨å®Œæ•´æ€§æª¢æŸ¥ã€ã€‚
* **ç”¨é€”**ï¼šåœ¨ CICD æµç¨‹ä¸­è‡ªå‹•ç¢ºèªå»è­˜åˆ¥åŒ–è³ªé‡æŒ‡æ¨™ï¼ˆPII ç§»é™¤ç‡ã€æ ¼å¼ä¿ç•™ç‡ï¼‰ã€‚

---

# EdgeDeID Studio å°ˆæ¡ˆå…¨åŠŸèƒ½å½™æ•´

## ä¸€ã€å°ˆæ¡ˆæ¶æ§‹èˆ‡æ ¸å¿ƒæµç¨‹

### å»è­˜åˆ¥åŒ– + å‡è³‡æ–™æ›¿æ›å®Œæ•´æµç¨‹

```mermaid
graph LR
    A[åŸå§‹æ–‡ä»¶] --> B[æ–‡ä»¶è§£æå™¨]
    B --> C[æ–‡å­—æå–]
    C --> D[PIIåµæ¸¬å¼•æ“]
    D --> E[å‡è³‡æ–™æ›¿æ›]
    E --> F[å»è­˜åˆ¥åŒ–è¼¸å‡º]

    subgraph æ–‡ä»¶é¡å‹
        A1[PDF] --> B
        A2[DOCX] --> B
        A3[åœ–åƒ] --> B
        A4[Excel] --> B
        A5[PPT] --> B
    end

    subgraph PIIåµæ¸¬
        D1[æ­£å‰‡åŒ¹é…] --> D
        D2[BERTæ¨¡å‹] --> D
        D3[è¤‡åˆåµæ¸¬å™¨] --> D
    end

    subgraph å‡è³‡æ–™ç³»çµ±
        G[GPT-2ç”Ÿæˆå™¨] --> E
        H[Fakerè³‡æ–™åº«] --> E
        I[ä¸€è‡´æ€§å¿«å–] --> E
    end

    F --> J[æ–‡å­—æª”æ¡ˆ]
    F --> K[PDFæª”æ¡ˆ]
    F --> L[åœ–åƒæª”æ¡ˆ]
    F --> M[çµæ§‹åŒ–å ±å‘Š]
```

### å„æ¨¡çµ„ä¸²æ¥æµç¨‹

1. **æ–‡ä»¶è§£æéšæ®µ** (`text_extractor.py`)
   - è¼¸å…¥ï¼šå„ç¨®æ ¼å¼æ–‡ä»¶ (PDF, DOCX, åœ–åƒ, Excel, PPT)
   - è™•ç†ï¼š
     - ä½¿ç”¨ `fitz` è™•ç† PDF
     - ä½¿ç”¨ `python-docx` è™•ç† DOCX
     - ä½¿ç”¨ `EasyOCR` è™•ç†åœ–åƒ
     - ä½¿ç”¨ `pandas` å’Œ `openpyxl` è™•ç† Excel
     - ä½¿ç”¨ `python-pptx` è™•ç† PPT
   - è¼¸å‡ºï¼šçµ±ä¸€æ–‡å­—æ ¼å¼ + ä½ç½®æ˜ å°„

2. **PIIåµæ¸¬éšæ®µ** (`composite.py`)
   - è¼¸å…¥ï¼šç´”æ–‡å­—å…§å®¹
   - è™•ç†ï¼š
     - èª¿ç”¨ RegexDetector (åŸºæ–¼ `regex_zh.yaml`)
     - èª¿ç”¨ BertONNXDetector (ONNX åŠ é€Ÿæ¨¡å‹)
     - è§£æ±ºå¯¦é«”é‡ç–Šè¡çª (å„ªå…ˆç´š: èº«åˆ†è­‰ > æ‰‹æ©Ÿ > å§“å...)
   - è¼¸å‡ºï¼šPII å¯¦é«”åˆ—è¡¨ (é¡å‹, ä½ç½®, åŸå§‹å€¼)

3. **å‡è³‡æ–™æ›¿æ›éšæ®µ** (`replacer.py` + `fake_provider.py`)
   - è¼¸å…¥ï¼šåŸå§‹æ–‡å­— + PII å¯¦é«”åˆ—è¡¨
   - è™•ç†ï¼š
     - ä½¿ç”¨ GPT-2 ç”Ÿæˆæƒ…å¢ƒæ„ŸçŸ¥å‡è³‡æ–™
     - Faker ä½œç‚ºå‚™ç”¨ç”Ÿæˆå™¨
     - å…¨åŸŸä¸€è‡´æ€§å¿«å–ç¢ºä¿ç›¸åŒåŸå§‹å€¼æ›¿æ›ç›¸åŒå‡å€¼
     - åå‘æ›¿æ›é¿å…ä½ç½®åç§»
   - è¼¸å‡ºï¼šå»è­˜åˆ¥åŒ–æ–‡å­— + æ›¿æ›äº‹ä»¶è¨˜éŒ„

4. **æ ¼å¼é‡å»ºéšæ®µ** (å„æ ¼å¼å°ˆç”¨è™•ç†å™¨)
   - æ–‡å­—/PDFï¼šç›´æ¥è¼¸å‡ºæ›¿æ›å¾Œå…§å®¹
   - åœ–åƒï¼šOCR åè½‰è™•ç† (æ›¿æ›æ–‡å­—å›å¯«åˆ°åŸåœ–)
   - Excel/PPTï¼šä¿ç•™åŸå§‹æ ¼å¼ï¼Œåƒ…æ›¿æ›æ–‡å­—å…§å®¹

## äºŒã€æ•æ„Ÿå‡è³‡æ–™ç”Ÿæˆèˆ‡æ‡‰ç”¨

### å‡è³‡æ–™ç”Ÿæˆæµç¨‹

```mermaid
graph TB
    A[è³‡æ–™ç”Ÿæˆæ§åˆ¶å™¨] --> B[é¸æ“‡PIIé¡å‹]
    B --> C[ç”ŸæˆåŸå§‹å€¼]
    C --> D[åµŒå…¥æ¨¡æ¿]
    D --> E[æ¸²æŸ“æ ¼å¼]
    E --> F[è¼¸å‡ºæª”æ¡ˆ]

    B -->|é¡å‹| C1[èº«åˆ†è­‰]
    B -->|é¡å‹| C2[æ‰‹æ©Ÿè™Ÿ]
    B -->|é¡å‹| C3[åœ°å€]
    B -->|é¡å‹| C4[ç—…æ­·è™Ÿ]

    D -->|æ¨¡æ¿| D1[åˆç´„æ›¸]
    D -->|æ¨¡æ¿| D2[é†«ç™‚å ±å‘Š]
    D -->|æ¨¡æ¿| D3[è²¡å‹™å ±è¡¨]

    E -->|æ ¼å¼| E1[PDF]
    E -->|æ ¼å¼| E2[Word]
    E -->|æ ¼å¼| E3[Excel]
    E -->|æ ¼å¼| E4[PPT]
    E -->|æ ¼å¼| E5[æƒæåœ–åƒ]
```

### é—œéµæŠ€è¡“å¯¦ç¾

1. **å°ç£å°ˆç”¨PIIç”Ÿæˆå™¨** (`generators.py`)
   - èº«åˆ†è­‰ç”Ÿæˆç®—æ³•ï¼š
     ```python
     def generate_tw_id():
         area_codes = "ABCDEFGHJKLMNPQRSTUVXYWZ"
         first_char = random.choice(area_codes)
         gender_code = random.choice(['1', '2'])
         random_digits = ''.join(str(random.randint(0, 9)) for _ in range(7)
         # è¨ˆç®—æª¢æŸ¥ç¢¼ (ç¬¦åˆå®˜æ–¹è¦å‰‡)
         # ... å®Œæ•´ç®—æ³•å¯¦ç¾ ...
         return f"{first_char}{gender_code}{random_digits}{check_digit}"
     ```

2. **èªå¢ƒæ„ŸçŸ¥å‡è³‡æ–™** (`fake_provider.py`)
   ```python
   def generate_contextual_fake(entity_type, original, context):
       prompt = f"åœ¨{context}ä¸­ï¼Œå°‡ã€{original}ã€æ›¿æ›ç‚ºåˆç†çš„{entity_type}:"
       return self.gpt2_generate(prompt)
   ```

3. **å¤šæ ¼å¼æ¸²æŸ“å¼•æ“** (`advanced_file_writers.py`)
   - å‹•æ…‹ç”Ÿæˆå°ˆæ¥­å…ƒç´ ï¼š
     - å ±å‘Šå¯¦é©—å®¤ï¼šPDFè¡¨æ ¼ã€åœ–è¡¨
     - python-docxï¼šWordæ ¼å¼æ§åˆ¶
     - PILï¼šæ¨¡æ“¬æƒææ–‡ä»¶ï¼ˆç´™å¼µç´‹ç†ã€å°ç« ã€ç°½åï¼‰

### æ¸¬è©¦è³‡æ–™æ‡‰ç”¨æµç¨‹

```mermaid
sequenceDiagram
    participant T as æ¸¬è©¦ç³»çµ±
    participant G as å‡è³‡æ–™ç”Ÿæˆå™¨
    participant P as DeID Pipeline
    participant R as æ¸¬è©¦å ±å‘Š

    T->>G: ç”Ÿæˆæ¸¬è©¦è³‡æ–™é›†(æ ¼å¼, æ•¸é‡)
    G->>T: è¿”å›è³‡æ–™é›†è·¯å¾‘
    loop æ¯å€‹æ–‡ä»¶
        T->>P: è™•ç†æ–‡ä»¶(è·¯å¾‘)
        P->>T: è¿”å›è™•ç†çµæœ
        T->>T: é©—è­‰çµæœ(åŸå§‹PII, æ›¿æ›ä¸€è‡´æ€§)
    end
    T->>R: ç”Ÿæˆæ¸¬è©¦å ±å‘Š
```

## ä¸‰ã€æ¸¬è©¦ç­–ç•¥èˆ‡å“è³ªä¿è­‰

### åˆ†å±¤æ¸¬è©¦é«”ç³»

| æ¸¬è©¦å±¤ç´š | æ¸¬è©¦å·¥å…· | é©—è­‰ç›®æ¨™ | å“è³ªæŒ‡æ¨™ |
|----------|----------|----------|----------|
| **å–®å…ƒæ¸¬è©¦** | pytest | æ¨¡çµ„åŠŸèƒ½æ­£ç¢ºæ€§ | åˆ†æ”¯è¦†è“‹ç‡ > 90% |
| **æ•´åˆæ¸¬è©¦** | è‡ªè¨‚æ¸¬è©¦æ¡†æ¶ | æ¨¡çµ„é–“å”ä½œ | æµç¨‹æˆåŠŸç‡ 100% |
| **ç«¯åˆ°ç«¯æ¸¬è©¦** | å‡è³‡æ–™ç”Ÿæˆå™¨ | çœŸå¯¦å ´æ™¯è™•ç† | PIIåµæ¸¬ç‡ > 95% |
| **æ•ˆèƒ½æ¸¬è©¦** | timeit + åˆ†æå™¨ | éŸ¿æ‡‰æ™‚é–“è³‡æºä½”ç”¨ | ONNXå»¶é² < 25ms |
| **å£“åŠ›æ¸¬è©¦** | å¤§è¦æ¨¡è³‡æ–™é›† | ç³»çµ±ç©©å®šæ€§ | è¨˜æ†¶é«”æº¢ä½ç‡ 0% |

### é—œéµæ¸¬è©¦æ¡ˆä¾‹å¯¦ç¾

1. **ç«¯åˆ°ç«¯æ¸¬è©¦** (`end_to_end_test.py`)
   ```python
   def test_pdf_deidentification():
       # ç”Ÿæˆæ¸¬è©¦PDF
       pdf_path = generate_contract_pdf()

       # è™•ç†æ–‡ä»¶
       result = deid_pipeline.process(pdf_path)

       # é©—è­‰çµæœ
       assert "A123456789" not in result.text
       assert result.format_preserved == True
       assert result.processing_time < 2.0  # 2ç§’å…§å®Œæˆ
   ```

2. **å‡è³‡æ–™æ•´åˆæ¸¬è©¦** (`test_data_generator_integration.py`)
   ```python
   def test_generator_pipeline_integration():
       # ç”Ÿæˆ100å€‹æ¸¬è©¦æ–‡ä»¶
       dataset = generate_test_dataset(num_items=100)

       detection_rates = []
       for item in dataset:
           # è™•ç†æ¯å€‹æ–‡ä»¶
           result = deid_pipeline.process(item['path'])

           # é©—è­‰åŸå§‹PIIæ˜¯å¦è¢«åµæ¸¬
           original_pii = extract_original_pii(item['content'])
           detected = all(pii in result.entities for pii in original_pii)
           detection_rates.append(detected)

       # è¨ˆç®—åµæ¸¬ç‡
       detection_rate = sum(detection_rates) / len(detection_rates)
       assert detection_rate >= 0.95  # 95%åµæ¸¬ç‡è¦æ±‚
   ```

3. **æ•ˆèƒ½åŸºæº–æ¸¬è©¦** (`test_onnx_speed.py`)
   ```python
   def test_onnx_inference_speed():
       # æº–å‚™é•·æ–‡æœ¬ (10kå­—å…ƒ)
       long_text = generate_long_text(10000)

       # æ¸¬è©¦ONNXæ¨¡å‹
       detector = BertONNXDetector()
       start_time = time.perf_counter()
       entities = detector.detect(long_text)
       elapsed = (time.perf_counter() - start_time) * 1000  # ms

       assert elapsed < 25  # 25msä»¥å…§
       assert len(entities) > 0  # ç¢ºä¿æœ‰åµæ¸¬çµæœ
   ```

## å››ã€å‰µæ–°æŠ€è¡“äº®é»

1. **ç¹é«”ä¸­æ–‡å°ˆå±¬è™•ç†**
   - å°ç£èº«åˆ†è­‰é©—è­‰ç®—æ³•
   - æœ¬åœ°åŒ–åœ°å€ç”Ÿæˆ (ç¸£å¸‚+è¡—é“+å··å¼„)
   - é†«ç™‚ç—…æ­·è™Ÿç¢¼æ ¼å¼æ¨¡æ“¬

2. **è·¨æ ¼å¼ä¸€è‡´æ€§è™•ç†**
   - çµ±ä¸€æ–‡å­—æå–ä»‹é¢
   - æ ¼å¼ç„¡é—œçš„PIIåµæ¸¬
   - å„æ ¼å¼å°ˆå±¬é‡å»ºæ©Ÿåˆ¶

3. **æ•ˆèƒ½å„ªåŒ–æŠ€è¡“**
   - ONNXæ¨¡å‹åŠ é€Ÿ (CPU/GPU/NPU)
   - æ»‘å‹•çª—å£è™•ç†é•·æ–‡æœ¬
   - ä¸¦è¡Œè™•ç†ç®¡é“

4. **æƒ…å¢ƒæ„ŸçŸ¥å‡è³‡æ–™**
   - GPT-2èªå¢ƒç”Ÿæˆ
   - å°ˆæ¥­é ˜åŸŸæ¨¡æ¿ (é†«ç™‚/æ³•å¾‹/è²¡å‹™)
   - å…¨åŸŸä¸€è‡´æ€§å¿«å–

5. **æ¸¬è©¦è‡ªå‹•åŒ–é«”ç³»**
   - å‡è³‡æ–™é©…å‹•æ¸¬è©¦
   - æ ¼å¼ç›¸å®¹æ€§é©—è­‰
   - æŒçºŒæ•´åˆç®¡é“

## äº”ã€ç³»çµ±è¼¸å‡ºèˆ‡å ±å‘Š

### DeIDè™•ç†çµæœç‰©ä»¶

```python
class DeidResult:
    def __init__(self):
        self.entities = []      # åµæ¸¬åˆ°çš„PIIå¯¦é«”
        self.text = ""           # è™•ç†å¾Œæ–‡å­— (æ–‡å­—æ ¼å¼)
        self.output_path = ""    # è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
        self.report = {          # è™•ç†å ±å‘Š
            "pii_count": 0,
            "processing_time": 0.0,
            "format_preserved": False,
            "replacement_map": {}
        }
        self.events = []        # è™•ç†äº‹ä»¶æ—¥èªŒ
```

### æ¸¬è©¦å ±å‘Šç¯„ä¾‹

```json
{
  "test_suite": "end_to_end",
  "timestamp": "2023-11-15T14:30:45Z",
  "statistics": {
    "total_files": 100,
    "success_rate": 98.0,
    "average_time": 1.24,
    "formats": {
      "pdf": {"count": 30, "success": 29, "avg_time": 1.8},
      "docx": {"count": 20, "success": 20, "avg_time": 1.2},
      "xlsx": {"count": 20, "success": 20, "avg_time": 1.5},
      "png": {"count": 30, "success": 29, "avg_time": 2.1}
    }
  },
  "issues": [
    {
      "file": "contract_45.pdf",
      "issue": "ç°½åå€åŸŸæœªè¢«æ­£ç¢ºåµæ¸¬",
      "resolution": "å¢åŠ ç°½ååµæ¸¬è¦å‰‡"
    }
  ],
  "quality_metrics": {
    "pii_detection_rate": 97.3,
    "replacement_consistency": 100.0,
    "format_preservation": 98.0
  }
}
```
=======
# å¾ venv å•Ÿå‹•è¨­è¨ˆç•«é¢(è¦è¨­è¨ˆ layout å†é–‹)
.venv\Scripts\pyside6-de

signer.exe
>>>>>>> 34dcda9fb5ae566dae229ce3b25eba7fece9da20
